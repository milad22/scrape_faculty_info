{
    "columns":[
        "abstract",
        "title",
        "author",
        "pub_year",
        "author_pub_id",
        "num_citations",
        "pub_url",
        "cites_id",
        "citedby_url",
        "cites_per_year"
    ],
    "index":[
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
    ],
    "data":[
        [
            "A new regression technique based on Vapnik's concept of support vectors is introduced. We compare support vector regression (SVR) with a committee regression technique (bagging) based on regression trees and ridge regression done in feature space. On the basis of these experiments, it is expected that SVR will have advantages in high dimensionality space because SVR optimization does not depend on the dimensionality of the input space.",
            "Support vector regression machines",
            "Harris Drucker and Chris JC Burges and Linda Kaufman and Alex Smola and Vladimir Vapnik",
            "1997",
            "5ZMeAxoAAAAJ:u5HHmVD_uO8C",
            3950,
            "http:\/\/books.google.com\/books?hl=en&lr=&id=QpD7n95ozWUC&oi=fnd&pg=PA155&dq=info:OFN8Rgs9PYMJ:scholar.google.com&ots=iEkqkHXXcA&sig=ERDogRN2HezHWVsxrJXk6sR5BYg",
            "9456781911184397112",
            "\/scholar?cites=9456781911184397112",
            {
                "1999":12,
                "2000":17,
                "2001":20,
                "2002":13,
                "2003":30,
                "2004":37,
                "2005":55,
                "2006":63,
                "2007":82,
                "2008":85,
                "2009":123,
                "2010":125,
                "2011":124,
                "2012":132,
                "2013":160,
                "2014":246,
                "2015":296,
                "2016":302,
                "2017":321,
                "2018":451,
                "2019":534,
                "2020":626,
                "2021":58
            }
        ],
        [
            "Positron emission tomography (PET)\u2014still in its research stages\u2014is a technique that promises to open new medical frontiers by enabling physicians to study the metabolic activity of the body in a pictorial manner. Much as in X-ray transmission tomography and other modes of computerized tomography, the quality of the reconstructed image in PET is very sensitive to the mathematical algorithm to be used for reconstruction. In this article, we tailor a mathematical model to the physics of positron emissions, and we use the model to describe the basic image reconstruction problem of PET as a standard problem in statistical estimation from incomplete data. We describe various estimation procedures, such as the maximum likelihood (ML) method (using the EM algorithm), the method of moments, and the least squares method. A computer simulation of a PET experiment is then used to demonstrate the ML and the least \u2026",
            "A statistical model for positron emission tomography",
            "Yehuda Vardi and LA Shepp and Linda Kaufman",
            "1985",
            "5ZMeAxoAAAAJ:u-x6o8ySG0sC",
            1131,
            "https:\/\/www.tandfonline.com\/doi\/abs\/10.1080\/01621459.1985.10477119",
            "15909094700264371942",
            "\/scholar?cites=15909094700264371942",
            {
                "1985":4,
                "1986":8,
                "1987":23,
                "1988":18,
                "1989":14,
                "1990":21,
                "1991":32,
                "1992":32,
                "1993":23,
                "1994":30,
                "1995":27,
                "1996":36,
                "1997":38,
                "1998":29,
                "1999":20,
                "2000":32,
                "2001":26,
                "2002":20,
                "2003":25,
                "2004":36,
                "2005":28,
                "2006":31,
                "2007":38,
                "2008":33,
                "2009":37,
                "2010":33,
                "2011":38,
                "2012":42,
                "2013":46,
                "2014":37,
                "2015":42,
                "2016":50,
                "2017":39,
                "2018":44,
                "2019":37,
                "2020":39,
                "2021":3
            }
        ],
        [
            "Numerical linear algebra, particularly the solution of linear systems of equations, linear least squares problems, eigenvalue problems and singular value",
            "An updated set of basic linear algebra subprograms (BLAS)",
            "L Susan Blackford and Antoine Petitet and Roldan Pozo and Karin Remington and R Clint Whaley and James Demmel and Jack Dongarra and Iain Duff and Sven Hammarling and Greg Henry and Michael Heroux and Linda Kaufman and Andrew Lumsdaine",
            "2002",
            "5ZMeAxoAAAAJ:9yKSN-GCB0IC",
            753,
            "https:\/\/redaron.com\/an_updated_set_of_basic.pdf",
            "8062358010227254241",
            "\/scholar?cites=8062358010227254241",
            {
                "2002":4,
                "2003":6,
                "2004":6,
                "2005":16,
                "2006":27,
                "2007":27,
                "2008":43,
                "2009":36,
                "2010":48,
                "2011":51,
                "2012":53,
                "2013":60,
                "2014":62,
                "2015":62,
                "2016":36,
                "2017":50,
                "2018":39,
                "2019":50,
                "2020":60,
                "2021":7
            }
        ],
        [
            "Numerically stable algorithms are given for updating the Gram-Schmidt QR factorization of an  matrix  when A is modified by a matrix of rank one, or when a row or column is inserted or deleted. The algorithms require  operations per update, and are based on the use of elementary two-by-two reflection matrices and the Gram-Schmidt process with reorthogonalization. An error analysis of the reorthogonalization process provides rigorous justification for the corresponding ALGOL procedures.",
            "Reorthogonalization and stable algorithms for updating the Gram-Schmidt \ud835\udc44\ud835\udc45 factorization",
            "James W Daniel and Walter Bill Gragg and Linda Kaufman and Gilbert W Stewart",
            "1976",
            "5ZMeAxoAAAAJ:d1gkVwhDpl0C",
            641,
            "https:\/\/www.ams.org\/mcom\/1976-30-136\/S0025-5718-1976-0431641-8\/",
            "4180987374455560927",
            "\/scholar?cites=4180987374455560927",
            {
                "1982":5,
                "1983":9,
                "1984":6,
                "1985":2,
                "1986":4,
                "1987":4,
                "1988":9,
                "1989":11,
                "1990":8,
                "1991":7,
                "1992":16,
                "1993":14,
                "1994":13,
                "1995":23,
                "1996":12,
                "1997":22,
                "1998":21,
                "1999":18,
                "2000":13,
                "2001":23,
                "2002":11,
                "2003":14,
                "2004":20,
                "2005":26,
                "2006":11,
                "2007":15,
                "2008":19,
                "2009":26,
                "2010":29,
                "2011":19,
                "2012":27,
                "2013":23,
                "2014":21,
                "2015":18,
                "2016":26,
                "2017":20,
                "2018":27,
                "2019":25,
                "2020":1
            }
        ],
        [
            "Several decompositions of symmetric matrices for calculating inertia and solving systems of linear equations are discussed. New partial pivoting strategies for decomposing symmetric matrices are introduced and analyzed.",
            "Some stable methods for calculating inertia and solving symmetric linear systems",
            "James R Bunch and Linda Kaufman",
            "1977",
            "5ZMeAxoAAAAJ:2osOgNQ5qMEC",
            515,
            "https:\/\/www.jstor.org\/stable\/2005787",
            "15606001814449177031",
            "\/scholar?cites=15606001814449177031",
            {
                "1982":9,
                "1983":2,
                "1984":2,
                "1985":3,
                "1986":6,
                "1987":8,
                "1988":4,
                "1989":5,
                "1990":8,
                "1991":8,
                "1992":8,
                "1993":8,
                "1994":11,
                "1995":10,
                "1996":17,
                "1997":16,
                "1998":10,
                "1999":14,
                "2000":12,
                "2001":13,
                "2002":5,
                "2003":12,
                "2004":17,
                "2005":20,
                "2006":13,
                "2007":8,
                "2008":8,
                "2009":19,
                "2010":21,
                "2011":25,
                "2012":24,
                "2013":23,
                "2014":18,
                "2015":26,
                "2016":30,
                "2017":14,
                "2018":17,
                "2019":17,
                "2020":1
            }
        ],
        [
            "Since the publication of Shepp and Vadi's [ 14] maximum likelihood reconstruction algorithm for emission tomography (ET), many medical research centers engaged in ET have made an effort to change their reconstruction algorithms to this new approach. Some have succeeded, while others claim they could not adopt this new approach primarily because of limited computing power. In this paper, we discuss techniques for reducing the computational requirements of the reconstruction algorithm. Specifically, the paper discusses the data structures one might use and ways of taking advantage of the geometry of the physical system. The paper also treats some of the numerical aspects of the EM (expectation maximization) algorithm, and ways of speeding up the numerical algorithm using some of the traditional techniques of numerical analysis.",
            "Implementing and accelerating the EM algorithm for positron emission tomography",
            "Linda Kaufman",
            "1987",
            "5ZMeAxoAAAAJ:qjMakFHDy7sC",
            343,
            "https:\/\/ieeexplore.ieee.org\/abstract\/document\/4307796\/",
            "11677975825071081138",
            "\/scholar?cites=11677975825071081138",
            {
                "1987":1,
                "1988":5,
                "1989":6,
                "1990":12,
                "1991":6,
                "1992":15,
                "1993":12,
                "1994":21,
                "1995":14,
                "1996":25,
                "1997":17,
                "1998":12,
                "1999":11,
                "2000":10,
                "2001":13,
                "2002":10,
                "2003":7,
                "2004":8,
                "2005":13,
                "2006":11,
                "2007":11,
                "2008":10,
                "2009":8,
                "2010":13,
                "2011":6,
                "2012":4,
                "2013":9,
                "2014":14,
                "2015":7,
                "2016":9,
                "2017":9,
                "2018":4,
                "2019":3,
                "2020":3
            }
        ],
        [
            "Consider the separable nonlinear least squares problem of findinga \u03b5R  n  and \u03b1 \u03b5R  k  which, for given data (y  i ,t  i ),i=1,2,...m, and functions \u03d5 j (\u03b1,t),j=1,2,...,n(m>n), minimize the functional   where \u03b8(\u03b1) ij =\u03d5 j (\u03b1,t  i ). Golub and Pereyra have shown that this problem can be reduced to a nonlinear least squares problem involving\u03b1 only, and a linear least squares problem involvinga only. In this paper we propose a new method for determining the optimal\u03b1 which computationally has proved more efficient than the Golub-Pereyra scheme.",
            "A variable projection method for solving separable nonlinear least squares problems",
            "Linda Kaufman",
            "1975",
            "5ZMeAxoAAAAJ:UeHWp8X0CEIC",
            271,
            "https:\/\/link.springer.com\/content\/pdf\/10.1007\/BF01932995.pdf",
            "8764387054200896929",
            "\/scholar?cites=8764387054200896929",
            {
                "1982":1,
                "1983":1,
                "1984":5,
                "1985":4,
                "1986":3,
                "1987":3,
                "1988":5,
                "1989":8,
                "1990":5,
                "1991":8,
                "1992":6,
                "1993":3,
                "1994":7,
                "1995":6,
                "1996":6,
                "1997":7,
                "1998":6,
                "1999":3,
                "2000":3,
                "2001":5,
                "2002":5,
                "2003":6,
                "2004":4,
                "2005":6,
                "2006":8,
                "2007":6,
                "2008":12,
                "2009":13,
                "2010":15,
                "2011":9,
                "2012":12,
                "2013":11,
                "2014":6,
                "2015":6,
                "2016":15,
                "2017":12,
                "2018":6,
                "2019":9,
                "2020":1
            }
        ],
        [
            "The EM algorithm is the basic approach used to maximize the log likelihood objective function for the reconstruction problem in positron emission tomography (PET). The EM algorithm is a scaled steepest ascent algorithm that elegantly handles the nonnegativity constraints of the problem. It is shown that the same scaled steepest descent algorithm can be applied to the least squares merit function, and that it can be accelerated using the conjugate gradient approach. The experiments suggest that one can cut the computation by about a factor of 3 by using this technique. The results are applied to various penalized least squares functions which might be used to produce a smoother image.< >",
            "Maximum likelihood, least squares, and penalized least squares for PET",
            "Linda Kaufman",
            "1993",
            "5ZMeAxoAAAAJ:IjCSPb-OGe4C",
            232,
            "https:\/\/ieeexplore.ieee.org\/abstract\/document\/232249\/",
            "12603202257272057808",
            "\/scholar?cites=12603202257272057808",
            {
                "1994":5,
                "1995":7,
                "1996":10,
                "1997":9,
                "1998":5,
                "1999":14,
                "2000":9,
                "2001":8,
                "2002":13,
                "2003":5,
                "2004":9,
                "2005":9,
                "2006":14,
                "2007":11,
                "2008":20,
                "2009":7,
                "2010":7,
                "2011":8,
                "2012":12,
                "2013":4,
                "2014":10,
                "2015":4,
                "2016":8,
                "2017":7,
                "2018":7,
                "2019":6,
                "2020":3
            }
        ],
        [
            "In machine learning one is given\u2019training\u2019vectors xi, i= 1,..., l of length N and integers ai, i= 1,..., l. which suggest that data xi is supposed to be associated with class ai. From this data one is supposed to create a mechanism by which one can determine for any given data item of length N which class it is in. As shown in Vapnik [12] central to the support vector technique is a matrix Q where qi, j is a function of xi and xj and essentially defines the surface that separates the classes. For class m define the vector y as follows yi=",
            "Solving the quadratic programming problem arising in support vector classification",
            "Linda Kaufman",
            "1998",
            "5ZMeAxoAAAAJ:zYLM7Y9cAGgC",
            155,
            "https:\/\/www.researchgate.net\/profile\/Linda_Kaufman3\/publication\/303446582_Solving_the_Quadratic_Programming_Problem_Arising_in_Support_Vector_Classication\/links\/5743748708ae9ace841b3a79\/Solving-the-Quadratic-Programming-Problem-Arising-in-Support-Vector-Classication.pdf",
            "706844472671656615",
            "\/scholar?cites=706844472671656615",
            {
                "1998":6,
                "1999":4,
                "2000":11,
                "2001":15,
                "2002":6,
                "2003":10,
                "2004":11,
                "2005":13,
                "2006":10,
                "2007":4,
                "2008":12,
                "2009":10,
                "2010":11,
                "2011":5,
                "2012":8,
                "2013":1,
                "2014":6,
                "2015":4,
                "2016":2,
                "2017":1,
                "2018":1,
                "2019":1
            }
        ],
        [
            "An algorithm is presented to compute a triangular factorization and the inertia of a symmetric matrix. The algorithm is stable even when the matrix is not positive definite and is as fast as Cholesky. Programs for solving associated systems of linear equations are included.",
            "Decomposition of a symmetric matrix",
            "James R Bunch and Linda Kaufman and Beresford N Parlett",
            "1976",
            "5ZMeAxoAAAAJ:Tyk-4Ss8FVUC",
            142,
            "https:\/\/link.springer.com\/content\/pdf\/10.1007\/BF01399088.pdf",
            "7440117818318307748",
            "\/scholar?cites=7440117818318307748",
            {
                "1982":4,
                "1983":1,
                "1984":2,
                "1985":1,
                "1986":3,
                "1987":1,
                "1988":6,
                "1989":3,
                "1990":4,
                "1991":3,
                "1992":3,
                "1993":4,
                "1994":8,
                "1995":5,
                "1996":4,
                "1997":7,
                "1998":2,
                "1999":5,
                "2000":4,
                "2001":7,
                "2002":5,
                "2003":3,
                "2004":3,
                "2005":8,
                "2006":9,
                "2007":3,
                "2008":2,
                "2009":4,
                "2010":2,
                "2011":3,
                "2012":7,
                "2013":2,
                "2014":4,
                "2015":2
            }
        ]
    ]
}
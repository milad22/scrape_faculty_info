{
    "columns":[
        "abstract",
        "title",
        "author",
        "pub_year",
        "author_pub_id",
        "num_citations",
        "pub_url",
        "cites_id",
        "citedby_url",
        "cites_per_year"
    ],
    "index":[
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
    ],
    "data":[
        [
            "LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.",
            "LIBLINEAR: A library for large linear classification",
            "Rong-En Fan and Kai-Wei Chang and Cho-Jui Hsieh and Xiang-Rui Wang and Chih-Jen Lin",
            "2008",
            "fqDBtzYAAAAJ:u5HHmVD_uO8C",
            8437,
            "https:\/\/www.jmlr.org\/papers\/volume9\/fan08a\/fan08a.pdf",
            "13994175380392112295",
            "\/scholar?cites=13994175380392112295",
            {
                "2009":101,
                "2010":253,
                "2011":359,
                "2012":537,
                "2013":663,
                "2014":911,
                "2015":1079,
                "2016":1101,
                "2017":958,
                "2018":896,
                "2019":789,
                "2020":595,
                "2021":35
            }
        ],
        [
            "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female\/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to\" debias\" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",
            "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
            "Tolga Bolukbasi and Kai-Wei Chang and James Y Zou and Venkatesh Saligrama and Adam T Kalai",
            "2016",
            "fqDBtzYAAAAJ:rTD5ala9j4wC",
            1161,
            "https:\/\/arxiv.org\/abs\/1607.06520",
            "1143892262062010100",
            "\/scholar?cites=1143892262062010100",
            {
                "2016":6,
                "2017":59,
                "2018":170,
                "2019":378,
                "2020":504,
                "2021":22
            }
        ],
        [
            "In many applications, data appear with a huge number of instances as well as features. Linear Support Vector Machines (SVM) is one of the most popular tools to deal with such large-scale sparse data. This paper presents a novel dual coordinate descent method for linear SVM with L1-and L2-loss functions. The proposed method is simple and reaches an \u03b5-accurate solution in O (log (1\/\u03b5)) iterations. Experiments indicate that our method is much faster than state of the art solvers such as Pegasos, TRON, SVM perf, and a recent primal coordinate descent implementation.",
            "A dual coordinate descent method for large-scale linear SVM",
            "Cho-Jui Hsieh and Kai-Wei Chang and Chih-Jen Lin and S Sathiya Keerthi and Sellamanickam Sundararajan",
            "2008",
            "fqDBtzYAAAAJ:u-x6o8ySG0sC",
            1021,
            "https:\/\/dl.acm.org\/doi\/abs\/10.1145\/1390156.1390208",
            "12774884118709801482",
            "\/scholar?cites=12774884118709801482",
            {
                "2008":7,
                "2009":37,
                "2010":51,
                "2011":63,
                "2012":73,
                "2013":103,
                "2014":88,
                "2015":73,
                "2016":108,
                "2017":116,
                "2018":93,
                "2019":104,
                "2020":72,
                "2021":8
            }
        ],
        [
            "Kernel techniques have long been used in SVM to handle linearly inseparable problems by transforming data to a high dimensional space, but training and testing large data sets is often time consuming. In contrast, we can efficiently train and test much larger data sets using linear SVM without kernels. In this work, we apply fast linear-SVM methods to the explicit form of polynomially mapped data and investigate implementation issues. The approach enjoys fast training and testing, but may sometimes achieve accuracy close to that of using highly nonlinear kernels. Empirical experiments show that the proposed method is useful for certain large-scale data sets. We successfully apply the proposed method to a natural language processing (NLP) application by improving the testing accuracy under some training\/testing speed requirements.",
            "Training and testing low-degree polynomial data mappings via linear SVM",
            "Yin-Wen Chang and Cho-Jui Hsieh and Kai-Wei Chang and Michael Ringgaard and Chih-Jen Lin",
            "2010",
            "fqDBtzYAAAAJ:UeHWp8X0CEIC",
            457,
            "https:\/\/www.jmlr.org\/papers\/volume11\/chang10a\/chang10a.pdf",
            "16788376595433697380",
            "\/scholar?cites=16788376595433697380",
            {
                "2010":4,
                "2011":13,
                "2012":16,
                "2013":34,
                "2014":37,
                "2015":37,
                "2016":55,
                "2017":54,
                "2018":73,
                "2019":72,
                "2020":51,
                "2021":7
            }
        ],
        [
            "Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively.",
            "Men also like shopping: Reducing gender bias amplification using corpus-level constraints",
            "Jieyu Zhao and Tianlu Wang and Mark Yatskar and Vicente Ordonez and Kai-Wei Chang",
            "2017",
            "fqDBtzYAAAAJ:k_7cPK9k7w8C",
            358,
            "https:\/\/arxiv.org\/abs\/1707.09457",
            "2445652374493118307",
            "\/scholar?cites=2445652374493118307",
            {
                "2017":4,
                "2018":66,
                "2019":123,
                "2020":145,
                "2021":10
            }
        ],
        [
            "Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classification and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while fixing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efficient and stable than state of the art methods such as Pegasos and TRON.",
            "Coordinate descent method for large-scale l2-loss linear support vector machines",
            "Kai-Wei Chang and Cho-Jui Hsieh and Chih-Jen Lin",
            "2008",
            "fqDBtzYAAAAJ:d1gkVwhDpl0C",
            276,
            "https:\/\/www.jmlr.org\/papers\/volume9\/chang08a\/chang08a.pdf",
            "10030768240123190865",
            "\/scholar?cites=10030768240123190865",
            {
                "2008":2,
                "2009":6,
                "2010":11,
                "2011":13,
                "2012":17,
                "2013":26,
                "2014":30,
                "2015":24,
                "2016":37,
                "2017":25,
                "2018":27,
                "2019":34,
                "2020":21,
                "2021":1
            }
        ],
        [
            "Large-scale linear classification is widely used in many areas. The L1-regularized form can be applied for feature selection; however, its non-differentiability causes more difficulties in training. Although various optimization methods have been proposed in recent years, these have not yet been compared suitably. In this paper, we first broadly review existing methods. Then, we discuss state-of-the-art software packages in detail and propose two efficient implementations. Extensive comparisons indicate that carefully implemented coordinate descent methods are very suitable for training large document data.",
            "A comparison of optimization methods and software for large-scale l1-regularized linear classification",
            "Guo-Xun Yuan and Kai-Wei Chang and Cho-Jui Hsieh and Chih-Jen Lin",
            "2010",
            "fqDBtzYAAAAJ:2osOgNQ5qMEC",
            263,
            "https:\/\/www.jmlr.org\/papers\/volume11\/yuan10c\/yuan10c.pdf",
            "164538133224471943",
            "\/scholar?cites=164538133224471943",
            {
                "2010":5,
                "2011":20,
                "2012":28,
                "2013":30,
                "2014":32,
                "2015":31,
                "2016":32,
                "2017":20,
                "2018":25,
                "2019":26,
                "2020":9
            }
        ],
        [
            "Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations are often virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97% and 70%, respectively. We additionally demonstrate that 92.3% of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.",
            "Generating natural language adversarial examples",
            "Moustafa Alzantot and Yash Sharma and Ahmed Elgohary and Bo-Jhang Ho and Mani Srivastava and Kai-Wei Chang",
            "2018",
            "fqDBtzYAAAAJ:sA9dB-pw3HoC",
            244,
            "https:\/\/arxiv.org\/abs\/1804.07998",
            "11405633361755931599",
            "\/scholar?cites=11405633361755931599",
            {
                "2018":6,
                "2019":64,
                "2020":163,
                "2021":11
            }
        ],
        [
            "Recent advances in linear classification have shown that for applications such as document classification, the training process can be extremely efficient. However, most of the existing training methods are designed by assuming that data can be stored in the computer memory. These methods cannot be easily applied to data larger than the memory capacity due to the random access to the disk. We propose and analyze a block minimization framework for data larger than the memory size. At each step a block of data is loaded from the disk and handled by certain learning methods. We investigate two implementations of the proposed framework for primal and dual SVMs, respectively. Because data cannot fit in memory, many design considerations are very different from those for traditional algorithms. We discuss and compare with existing approaches that are able to handle data larger than memory. Experiments \u2026",
            "Large linear classification when data cannot fit in memory",
            "Hsiang-Fu Yu and Cho-Jui Hsieh and Kai-Wei Chang and Chih-Jen Lin",
            "2010",
            "fqDBtzYAAAAJ:qjMakFHDy7sC",
            189,
            "https:\/\/dl.acm.org\/doi\/abs\/10.1145\/2086737.2086743",
            "8036429468914111216",
            "\/scholar?cites=8036429468914111216",
            {
                "2011":15,
                "2012":27,
                "2013":26,
                "2014":20,
                "2015":23,
                "2016":14,
                "2017":15,
                "2018":17,
                "2019":18,
                "2020":6
            }
        ],
        [
            "We introduce a new benchmark, WinoBias, for coreference resolution focused on gender bias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (eg the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing coreference benchmark datasets. Our dataset and code are available at this http URL.",
            "Gender bias in coreference resolution: Evaluation and debiasing methods",
            "Jieyu Zhao and Tianlu Wang and Mark Yatskar and Vicente Ordonez and Kai-Wei Chang",
            "2018",
            "fqDBtzYAAAAJ:silx2ntsSuwC",
            180,
            "https:\/\/arxiv.org\/abs\/1804.06876",
            "9531484481211225588",
            "\/scholar?cites=9531484481211225588",
            {
                "2017":1,
                "2018":13,
                "2019":56,
                "2020":101,
                "2021":7
            }
        ]
    ]
}
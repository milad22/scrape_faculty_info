{
    "columns":[
        "abstract",
        "title",
        "author",
        "pub_year",
        "author_pub_id",
        "num_citations",
        "pub_url",
        "cites_id",
        "citedby_url",
        "cites_per_year"
    ],
    "index":[
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
    ],
    "data":[
        [
            "Implementing compact, low-power artificial neural processing systems with real-time on-line learning abilities is still an open challenge. In this paper we present a full-custom mixed-signal VLSI device with neuromorphic learning circuits that emulate the biophysics of real spiking neurons and dynamic synapses for exploring the properties of computational neuroscience models and for building brain-inspired computing systems. The proposed architecture allows the on-chip configuration of a wide range of network connectivities, including recurrent and deep networks with short-term and long-term plasticity. The device comprises 128 K analog synapse and 256 neuron circuits with biologically plausible dynamics and bi-stable spike-based plasticity mechanisms that endow it with on-line learning abilities. In addition to the analog circuits, the device comprises also asynchronous digital logic circuits for setting different synapse and neuron properties as well as different network configurations. This prototype device, fabricated using a 180 nm 1P6M CMOS process, occupies an area of 51.4 mm 2 , and consumes approximately 4 mW for typical experiments, for example involving attractor networks. Here we describe the details of the overall architecture and of the individual circuits and present experimental results that showcase its potential. By supporting a wide range of cortical-like computational modules comprising plasticity mechanisms, this device will enable the realization of intelligent autonomous systems with on-line learning capabilities.",
            "A reconfigurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128K synapses",
            "Ning Qiao and Hesham Mostafa and Federico Corradi and Marc Osswald and Fabio Stefanini and Dora Sumislawska and Giacomo Indiveri",
            "2015",
            "gXuEJfsAAAAJ:d1gkVwhDpl0C",
            396,
            "https:\/\/www.frontiersin.org\/articles\/10.3389\/fnins.2015.00141\/full",
            "8405375842631263888",
            "\/scholar?cites=8405375842631263888",
            {
                "2015":9,
                "2016":36,
                "2017":80,
                "2018":83,
                "2019":88,
                "2020":93,
                "2021":5
            }
        ],
        [
            "Gradient descent training techniques are remarkably successful in training analog-valued artificial neural networks (ANNs). Such training techniques, however, do not transfer easily to spiking networks due to the spike generation hard nonlinearity and the discrete nature of spike communication. We show that in a feedforward spiking network that uses a temporal coding scheme where information is encoded in spike times instead of spike rates, the network input-output relation is differentiable almost everywhere. Moreover, this relation is piecewise linear after a transformation of variables. Methods for training ANNs thus carry directly to the training of such spiking networks as we show when training on the permutation invariant MNIST task. In contrast to rate-based spiking networks that are often used to approximate the behavior of ANNs, the networks we present spike much more sparsely and their behavior cannot \u2026",
            "Supervised learning based on temporal coding in spiking neural networks",
            "Hesham Mostafa",
            "2017",
            "gXuEJfsAAAAJ:Y0pCki6q_DkC",
            139,
            "https:\/\/ieeexplore.ieee.org\/abstract\/document\/7999227\/",
            "11642829514204190384",
            "\/scholar?cites=11642829514204190384",
            {
                "2016":1,
                "2017":6,
                "2018":17,
                "2019":47,
                "2020":62,
                "2021":6
            }
        ],
        [
            "Convolutional neural networks (CNNs) have become the dominant neural network architecture for solving many state-of-the-art (SOA) visual processing tasks. Even though graphical processing units are most often used in training and deploying CNNs, their power efficiency is less than 10 GOp\/s\/W for single-frame runtime inference. We propose a flexible and efficient CNN accelerator architecture called NullHop that implements SOA CNNs useful for low-power and low-latency application scenarios. NullHop exploits the sparsity of neuron activations in CNNs to accelerate the computation and reduce memory requirements. The flexible architecture allows high utilization of available computing resources across kernel sizes ranging from 1\u00d71 to 7\u00d77. NullHop can process up to 128 input and 128 output feature maps per layer in a single pass. We implemented the proposed architecture on a Xilinx Zynq field \u2026",
            "Nullhop: A flexible convolutional neural network accelerator based on sparse representations of feature maps",
            "Alessandro Aimar and Hesham Mostafa and Enrico Calabrese and Antonio Rios-Navarro and Ricardo Tapiador-Morales and Iulia-Alexandra Lungu and Moritz B Milde and Federico Corradi and Alejandro Linares-Barranco and Shih-Chii Liu and Tobi Delbruck",
            "2018",
            "gXuEJfsAAAAJ:WF5omc3nYNoC",
            121,
            "https:\/\/ieeexplore.ieee.org\/abstract\/document\/8421093\/",
            "17881827776034249142",
            "\/scholar?cites=17881827776034249142",
            {
                "2017":5,
                "2018":11,
                "2019":39,
                "2020":60,
                "2021":5
            }
        ],
        [
            "Spiking neural networks (SNNs) are nature's versatile solution to fault-tolerant, energy-efficient signal processing. To translate these benefits into hardware, a growing number of neuromorphic spiking NN processors have attempted to emulate biological NNs. These developments have created an imminent need for methods and tools that enable such systems to solve real-world signal processing problems. Like conventional NNs, SNNs can be trained on real, domain-specific data; however, their training requires the overcoming of a number of challenges linked to their binary and dynamical nature. This article elucidates step-by-step the problems typically encountered when training SNNs and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting. Accordingly, it gives an overview of existing approaches and provides an introduction to surrogate gradient (SG \u2026",
            "Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks",
            "Emre O Neftci and Hesham Mostafa and Friedemann Zenke",
            "2019",
            "gXuEJfsAAAAJ:Zph67rFs4hoC",
            120,
            "https:\/\/ieeexplore.ieee.org\/abstract\/document\/8891809\/",
            "9966920258719430144",
            "\/scholar?cites=9966920258719430144",
            {
                "2019":21,
                "2020":91,
                "2021":7
            }
        ],
        [
            "CMOS technology and its sustainable scaling have been the enablers for the design and manufacturing of computer architectures that have been fuelling a wider range of applications. Today, however, both the technology and the computer architectures are suffering from serious challenges\/ walls making them incapable to deliver the right computing power at pre-defined constraints. This motivates the need of exploring new architectures and new technologies; not only to maintain the economic benefit of scaling, but also to enable the solutions of emerging computer power and data storage hungry applications such as big-data and data-intensive applications. This paper discusses the emerging memristor device as complementary (or alternative) to CMOS device and shows how this device can enable new ways of computing that will at least solve the challenges of today's architectures for some applications. The \u2026",
            "Memristor for computing: Myth or reality?",
            "Said Hamdioui and Shahar Kvatinsky and Gert Cauwenberghs and Lei Xie and Nimrod Wald and Siddharth Joshi and Hesham Mostafa Elsayed and Henk Corporaal and Koen Bertels",
            "2017",
            "gXuEJfsAAAAJ:_FxGoFyzp5QC",
            59,
            "https:\/\/ieeexplore.ieee.org\/abstract\/document\/7927083\/",
            "17165826674954014750",
            "\/scholar?cites=17165826674954014750",
            {
                "2017":16,
                "2018":8,
                "2019":18,
                "2020":16
            }
        ],
        [
            "Modern deep neural networks are typically highly overparameterized. Pruning techniques are able to remove a significant fraction of network parameters with little loss in accuracy. Recently, techniques based on dynamic reallocation of non-zero parameters have emerged, allowing direct training of sparse networks without having to pre-train a large dense model. Here we present a novel dynamic sparse reparameterization method that addresses the limitations of previous techniques such as high computational cost and the need for manual configuration of the number of free parameters allocated to each layer. We evaluate the performance of dynamic reallocation methods in training deep convolutional networks and show that our method outperforms previous static and dynamic reparameterization methods, yielding the best accuracy for a fixed parameter budget, on par with accuracies obtained by iteratively pruning a pre-trained dense model. We further investigated the mechanisms underlying the superior generalization performance of the resultant sparse networks. We found that neither the structure, nor the initialization of the non-zero parameters were sufficient to explain the superior performance. Rather, effective learning crucially depended on the continuous exploration of the sparse network structure space during training. Our work suggests that exploring structural degrees of freedom during training is more effective than adding extra parameters to the network.",
            "Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization",
            "Hesham Mostafa and Xin Wang",
            "2019",
            "gXuEJfsAAAAJ:9ZlFYXVOiuMC",
            48,
            "http:\/\/proceedings.mlr.press\/v97\/mostafa19a.html",
            "3342252922777294975",
            "\/scholar?cites=3342252922777294975",
            {
                "2019":8,
                "2020":35,
                "2021":5
            }
        ],
        [
            "A growing body of work underlines striking similarities between biological neural networks and recurrent, binary neural networks. A relatively smaller body of work, however, discusses similarities between learning dynamics employed in deep Artificial Neural Network and synaptic plasticity in spiking neural networks. The challenge preventing this is largely caused by the discrepancy between the dynamical properties of synaptic plasticity and the requirements for gradient backpropagation. Learning algorithms that approximate gradient backpropagation using locally synthesized gradients can overcome this challenge. Here, we show that synthetic gradients enable the derivation of Deep Continuous Local Learning (DECOLLE) in spiking neural networks. DECOLLE is capable of learning deep spatio-temporal representations from spikes relying solely on local information. Synaptic plasticity rules are derived systematically from user-defined cost functions and neural dynamics by leveraging existing autodifferentiation methods of machine learning frameworks. We benchmark our approach on the event-based neuromorphic dataset N-MNIST and DvsGesture, on which DECOLLE performs comparably to the state-of-the-art. DECOLLE networks provide continuously learning machines that are relevant to biology and supportive of event-based, low-power computer vision architectures matching the accuracies of conventional computers on tasks where temporal precision and speed are essential.",
            "Synaptic plasticity dynamics for deep continuous local learning (DECOLLE)",
            "Jacques Kaiser and Hesham Mostafa and Emre Neftci",
            "2020",
            "gXuEJfsAAAAJ:MXK_kJrjxJIC",
            40,
            "https:\/\/www.frontiersin.org\/articles\/10.3389\/fnins.2020.00424\/full?report=reader",
            "16312508685796081606",
            "\/scholar?cites=16312508685796081606",
            {
                "2019":13,
                "2020":25,
                "2021":1
            }
        ],
        [
            "Error backpropagation is a highly effective mechanism for learning high-quality hierarchical features in deep networks. Updating the features or weights in one layer, however, requires waiting for the propagation of error signals from higher layers. Learning using delayed and non-local errors makes it hard to reconcile backpropagation with the learning mechanisms observed in biological neural networks as it requires the neurons to maintain a memory of the input long enough until the higher-layer errors arrive. In this paper, we propose an alternative learning mechanism where errors are generated locally in each layer using fixed, random auxiliary classifiers. Lower layers could thus be trained independently of higher layers and training could either proceed layer by layer, or simultaneously in all layers using local error information. We address biological plausibility concerns such as weight symmetry requirements and show that the proposed learning mechanism based on fixed, broad, and random tuning of each neuron to the classification categories outperforms the biologically-motivated feedback alignment learning technique on the CIFAR10 dataset, approaching the performance of standard backpropagation. Our approach highlights a potential biological mechanism for the supervised, or task-dependent, learning of feature hierarchies. In addition, we show that it is well suited for learning deep networks in custom hardware where it can drastically reduce memory traffic and data communication overheads. Code used to run all learning experiments is available under \\url{https:\/\/gitlab.com\/hesham-mostafa\/learning-using-local-erros.git}",
            "Deep supervised learning using local errors",
            "Hesham Mostafa and Vishwajith Ramesh and Gert Cauwenberghs",
            "2018",
            "gXuEJfsAAAAJ:hqOjcs7Dif8C",
            40,
            "https:\/\/www.frontiersin.org\/articles\/10.3389\/fnins.2018.00608\/full",
            "1583988485332090910",
            "\/scholar?cites=1583988485332090910",
            {
                "2017":1,
                "2018":4,
                "2019":12,
                "2020":18,
                "2021":4
            }
        ],
        [
            "Synaptic plasticity plays a crucial role in allowing neural networks to learn and adapt to various input environments. Neuromorphic systems need to implement plastic synapses to obtain basic 'cognitive' capabilities such as learning. One promising and scalable approach for implementing neuromorphic synapses is to use nano-scale memristors as synaptic elements. In this paper we propose a hybrid CMOS-memristor system comprising CMOS neurons interconnected through TiO memristors, and spike-based learning circuits that modulate the conductance of the memristive synapse elements according to a spike-based Perceptron plasticity rule. We highlight a number of advantages for using this spike-based plasticity rule as compared to other forms of spike timing dependent plasticity (STDP) rules. We provide experimental proof-of-concept results with two silicon neurons connected through a memristive synapse that show how the CMOS plasticity circuits can induce stable changes in memristor conductances, giving rise to increased synaptic strength after a potentiation episode and to decreased strength after a depression episode.",
            "Implementation of a spike-based perceptron learning rule using TiO2\u2212 x memristors",
            "Hesham Mostafa and Ali Khiat and Alexander Serb and Christian G Mayr and Giacomo Indiveri and Themis Prodromakis",
            "2015",
            "gXuEJfsAAAAJ:zYLM7Y9cAGgC",
            40,
            "https:\/\/www.frontiersin.org\/articles\/10.3389\/fnins.2015.00357\/full",
            "12577172592538294622",
            "\/scholar?cites=12577172592538294622",
            {
                "2016":10,
                "2017":7,
                "2018":11,
                "2019":4,
                "2020":6,
                "2021":2
            }
        ],
        [
            "Constraint satisfaction problems are ubiquitous in many domains. They are typically solved using conventional digital computing architectures that do not reflect the distributed nature of many of these problems, and are thus ill-suited for solving them. Here we present a parallel analogue\/digital hardware architecture specifically designed to solve such problems. We cast constraint satisfaction problems as networks of stereotyped nodes that communicate using digital pulses, or events. Each node contains an oscillator implemented using analogue circuits. The non-repeating phase relations among the oscillators drive the exploration of the solution space. We show that this hardware architecture can yield state-of-the-art performance on random SAT problems under reasonable assumptions on the implementation. We present measurements from a prototype electronic chip to demonstrate that a physical \u2026",
            "An event-based architecture for solving constraint satisfaction problems",
            "Hesham Mostafa and Lorenz K M\u00fcller and Giacomo Indiveri",
            "2015",
            "gXuEJfsAAAAJ:qjMakFHDy7sC",
            33,
            "https:\/\/www.nature.com\/articles\/ncomms9941",
            "17306252029831318360",
            "\/scholar?cites=17306252029831318360",
            {
                "2015":1,
                "2016":3,
                "2017":11,
                "2018":5,
                "2019":4,
                "2020":9
            }
        ]
    ]
}
{
    "columns":[
        "abstract",
        "title",
        "author",
        "pub_year",
        "author_pub_id",
        "num_citations",
        "pub_url",
        "cites_id",
        "citedby_url",
        "cites_per_year"
    ],
    "index":[
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
    ],
    "data":[
        [
            "What does it mean to say that a certain set of spikes is the right answer to a computational problem? In what sense does a spike train convey information about the sensory world? Spikes begins by providing precise formulations of these and related questions about the representation of sensory signals in neural spike trains. The answers to these questions are then pursued in experiments on sensory neurons. Intended for neurobiologists with an interest in mathematical analysis of neural data as well as the growing number of physicists and mathematicians interested in information processing by\" real\" nervous systems, Spikes provides a self-contained review of relevant concepts in information theory and statistical decision theory. Our perception of the world is driven by input from the sensory nerves. This input arrives encoded as sequences of identical spikes. Much of neural computation involves processing these spike trains. What does it mean to say that a certain set of spikes is the right answer to a computational problem? In what sense does a spike train convey information about the sensory world? Spikes begins by providing precise formulations of these and related questions about the representation of sensory signals in neural spike trains. The answers to these questions are then pursued in experiments on sensory neurons. The authors invite the reader to play the role of a hypothetical observer inside the brain who makes decisions based on the incoming spike trains. Rather than asking how a neuron responds to a given stimulus, the authors ask how the brain could make inferences about an unknown stimulus from a given neural \u2026",
            "Spikes: Exploring the neural code",
            "F Rieke and D Warland and R De Ruyter van Steveninck and W Bialek",
            "1997",
            "fQ-BSlgAAAAJ:qUcmZB5y_30C",
            3619,
            "http:\/\/scholar.google.com\/scholar?cluster=9090819478669086053&hl=en&oi=scholarr",
            "9090819478669086053",
            "\/scholar?cites=9090819478669086053",
            {
                "1997":33,
                "1998":63,
                "1999":100,
                "2000":111,
                "2001":172,
                "2002":165,
                "2003":168,
                "2004":162,
                "2005":157,
                "2006":186,
                "2007":210,
                "2008":196,
                "2009":178,
                "2010":186,
                "2011":210,
                "2012":162,
                "2013":195,
                "2014":140,
                "2015":137,
                "2016":120,
                "2017":150,
                "2018":122,
                "2019":119,
                "2020":113,
                "2021":11
            }
        ],
        [
            "We define the relevant information in a signal  as being the information that this signal provides about another signal $ y\\in\\Y $. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal  requires more than just predicting , it also requires specifying which features of $\\X $ play a role in the prediction. We formalize this problem as that of finding a short code for $\\X $ that preserves the maximum information about $\\Y $. That is, we squeeze the information that $\\X $ provides about $\\Y $ through abottleneck'formed by a limited set of codewords $\\tX $. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure $ d (x,\\x) $ emerges from the joint statistics of $\\X $ and $\\Y $. This approach yields an exact set of self consistent equations for the coding rules $ X\\to\\tX $ and $\\tX\\to\\Y $. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.",
            "The information bottleneck method",
            "Naftali Tishby and Fernando C Pereira and William Bialek",
            "2000",
            "fQ-BSlgAAAAJ:u-x6o8ySG0sC",
            2265,
            "https:\/\/arxiv.org\/abs\/physics\/0004057",
            "17816044817750758700",
            "\/scholar?cites=17816044817750758700",
            {
                "2000":7,
                "2001":23,
                "2002":41,
                "2003":70,
                "2004":65,
                "2005":80,
                "2006":70,
                "2007":89,
                "2008":91,
                "2009":95,
                "2010":118,
                "2011":77,
                "2012":85,
                "2013":127,
                "2014":92,
                "2015":87,
                "2016":100,
                "2017":145,
                "2018":204,
                "2019":259,
                "2020":295,
                "2021":23
            }
        ],
        [
            "Biological networks have so many possible states that exhaustive sampling is impossible. Successful analysis thus depends on simplifying hypotheses, but experiments on many systems hint that complicated, higher-order interactions among large groups of elements have an important role. Here we show, in the vertebrate retina, that weak correlations between pairs of neurons coexist with strongly collective behaviour in the responses of ten or more neurons. We find that this collective behaviour is described quantitatively by models that capture the observed pairwise correlations but assume no higher-order interactions. These maximum entropy models are equivalent to Ising models, and predict that larger networks are completely dominated by correlation effects. This suggests that the neural code has associative or error-correcting properties, and we provide preliminary evidence for such behaviour. As a first test \u2026",
            "Weak pairwise correlations imply strongly correlated network states in a neural population",
            "Elad Schneidman and Michael J Berry and Ronen Segev and William Bialek",
            "2006",
            "fQ-BSlgAAAAJ:2osOgNQ5qMEC",
            1599,
            "https:\/\/www.nature.com\/articles\/nature04701",
            "3499093978246315979",
            "\/scholar?cites=3499093978246315979",
            {
                "2006":10,
                "2007":50,
                "2008":65,
                "2009":94,
                "2010":96,
                "2011":120,
                "2012":84,
                "2013":132,
                "2014":129,
                "2015":137,
                "2016":116,
                "2017":160,
                "2018":126,
                "2019":122,
                "2020":128,
                "2021":10
            }
        ],
        [
            "Traditional approaches to neural coding characterize the encoding of known stimuli in average neural responses. Organisms face nearly the opposite task--extracting information about an unknown time-dependent stimulus from short segments of a spike train. Here the neural code was characterized from the point of view of the organism, culminating in algorithms for real-time stimulus estimation based on a single example of the spike train. These methods were applied to an identified movement-sensitive neuron in the fly visual system. Such decoding experiments determined the effective noise level and fault tolerance of neural computation, and the structure of the decoding algorithms suggested a simple model for real-time analog signal processing with spiking neurons.",
            "Reading a neural code",
            "William Bialek and Fred Rieke and RR De Ruyter Van Steveninck and David Warland",
            "1991",
            "fQ-BSlgAAAAJ:u5HHmVD_uO8C",
            1226,
            "https:\/\/science.sciencemag.org\/content\/252\/5014\/1854.abstract",
            "9888408790689901978",
            "\/scholar?cites=9888408790689901978",
            {
                "1991":6,
                "1992":14,
                "1993":15,
                "1994":18,
                "1995":19,
                "1996":43,
                "1997":37,
                "1998":35,
                "1999":39,
                "2000":29,
                "2001":51,
                "2002":53,
                "2003":25,
                "2004":47,
                "2005":52,
                "2006":38,
                "2007":40,
                "2008":66,
                "2009":31,
                "2010":48,
                "2011":58,
                "2012":57,
                "2013":62,
                "2014":35,
                "2015":45,
                "2016":47,
                "2017":58,
                "2018":43,
                "2019":50,
                "2020":36,
                "2021":1
            }
        ],
        [
            "We study the statistics of an ensemble of images taken in the woods. Distributions of local quantities such as contrast are scale invariant and have nearly exponential tails. Power spectra exhibit scaling with a nontrivial exponent. These data limit the information content of natural images and point to the importance of gain-control strategies in visual processing.",
            "Statistics of natural images: Scaling in the woods",
            "Daniel L Ruderman and William Bialek",
            "1994",
            "fQ-BSlgAAAAJ:d1gkVwhDpl0C",
            1046,
            "https:\/\/journals.aps.org\/prl\/abstract\/10.1103\/PhysRevLett.73.814",
            "6100173604508980785",
            "\/scholar?cites=6100173604508980785",
            {
                "1994":8,
                "1995":8,
                "1996":15,
                "1997":21,
                "1998":20,
                "1999":30,
                "2000":38,
                "2001":35,
                "2002":20,
                "2003":42,
                "2004":34,
                "2005":43,
                "2006":39,
                "2007":50,
                "2008":47,
                "2009":57,
                "2010":52,
                "2011":43,
                "2012":47,
                "2013":49,
                "2014":59,
                "2015":57,
                "2016":47,
                "2017":55,
                "2018":51,
                "2019":37,
                "2020":30,
                "2021":2
            }
        ],
        [
            "The nervous system represents time dependent signals in sequences of discrete, identical action potentials or spikes; information is carried only in the spike arrival times. We show how to quantify this information, in bits, free from any assumptions about which features of the spike train or input signal are most important, and we apply this approach to the analysis of experiments on a motion sensitive neuron in the fly visual system. This neuron transmits information about the visual stimulus at rates of up to 90 bits\/s, within a factor of 2 of the physical limit set by the entropy of the spike train itself.",
            "Entropy and information in neural spike trains",
            "Steven P Strong and Roland Koberle and Rob R de Ruyter van Steveninck and William Bialek",
            "1998",
            "fQ-BSlgAAAAJ:9yKSN-GCB0IC",
            987,
            "https:\/\/journals.aps.org\/prl\/abstract\/10.1103\/PhysRevLett.80.197",
            "18030712511737164673",
            "\/scholar?cites=18030712511737164673",
            {
                "1999":14,
                "2000":21,
                "2001":40,
                "2002":23,
                "2003":40,
                "2004":42,
                "2005":36,
                "2006":43,
                "2007":44,
                "2008":56,
                "2009":34,
                "2010":55,
                "2011":59,
                "2012":57,
                "2013":56,
                "2014":37,
                "2015":55,
                "2016":50,
                "2017":52,
                "2018":48,
                "2019":63,
                "2020":46,
                "2021":2
            }
        ],
        [
            "We examine the dynamics of a neural code in the context of stimuli whose statistical properties are themselves evolving dynamically. Adaptation to these statistics occurs over a wide range of timescales\u2014from tens of milliseconds to minutes. Rapid components of adaptation serve to optimize the information that action potentials carry about rapid stimulus variations within the local statistical ensemble, while changes in the rate and statistics of action-potential firing encode information about the ensemble itself, thus resolving potential ambiguities. The speed with which information is optimized and ambiguities are resolved approaches the physical limit imposed by statistical sampling and noise.",
            "Efficiency and ambiguity in an adaptive neural code",
            "Adrienne L Fairhall and Geoffrey D Lewen and William Bialek and Robert R de Ruyter van Steveninck",
            "2001",
            "fQ-BSlgAAAAJ:qjMakFHDy7sC",
            804,
            "https:\/\/www.nature.com\/articles\/35090500",
            "16382135857568142103",
            "\/scholar?cites=16382135857568142103",
            {
                "2001":6,
                "2002":20,
                "2003":31,
                "2004":30,
                "2005":35,
                "2006":38,
                "2007":55,
                "2008":36,
                "2009":30,
                "2010":42,
                "2011":33,
                "2012":56,
                "2013":48,
                "2014":42,
                "2015":57,
                "2016":53,
                "2017":43,
                "2018":44,
                "2019":50,
                "2020":47
            }
        ],
        [
            "Adaptation is a widespread phenomenon in nervous systems, providing flexibility to function under varying external conditions. Here, we relate an adaptive property of a sensory system directly to its function as a carrier of information about input signals. We show that the input\/output relation of a sensory system in a dynamic environment changes with the statistical properties of the environment. Specifically, when the dynamic range of inputs changes, the input\/output relation rescales so as to match the dynamic range of responses to that of the inputs. We give direct evidence that the scaling of the input\/output relation is set to maximize information transmission for each distribution of signals. This adaptive behavior should be particularly useful in dealing with the intermittent statistics of natural signals.",
            "Adaptive rescaling maximizes information transmission",
            "Naama Brenner and William Bialek and Rob de Ruyter Van Steveninck",
            "2000",
            "fQ-BSlgAAAAJ:IjCSPb-OGe4C",
            693,
            "https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0896627300812052",
            "1395484813148577473",
            "\/scholar?cites=1395484813148577473",
            {
                "2000":3,
                "2001":17,
                "2002":18,
                "2003":29,
                "2004":24,
                "2005":34,
                "2006":34,
                "2007":49,
                "2008":42,
                "2009":35,
                "2010":36,
                "2011":36,
                "2012":38,
                "2013":38,
                "2014":38,
                "2015":30,
                "2016":36,
                "2017":38,
                "2018":43,
                "2019":33,
                "2020":35
            }
        ],
        [
            "The reproducibility and precision of biological patterning is limited by the accuracy with which concentration profiles of morphogen molecules can be established and read out by their targets. We consider four measures of precision for the Bicoid morphogen in the Drosophila embryo: the concentration differences that distinguish neighboring cells, the limits set by the random arrival of Bicoid molecules at their targets (which depends on absolute concentration), the noise in readout of Bicoid by the activation of Hunchback, and the reproducibility of Bicoid concentration at corresponding positions in multiple embryos. We show, through a combination of different experiments, that all of these quantities are \u223c10%. This agreement among different measures of accuracy indicates that the embryo is not faced with noisy input signals and readout mechanisms; rather, the system exerts precise control over absolute \u2026",
            "Probing the limits to positional information",
            "Thomas Gregor and David W Tank and Eric F Wieschaus and William Bialek",
            "2007",
            "fQ-BSlgAAAAJ:Tyk-4Ss8FVUC",
            607,
            "https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0092867407006629",
            "17584988705351880172",
            "\/scholar?cites=17584988705351880172",
            {
                "2007":16,
                "2008":43,
                "2009":46,
                "2010":44,
                "2011":51,
                "2012":54,
                "2013":42,
                "2014":52,
                "2015":45,
                "2016":54,
                "2017":28,
                "2018":35,
                "2019":37,
                "2020":52,
                "2021":4
            }
        ],
        [
            "Many of life\u2019s most fascinating phenomena emerge from interactions among many elements\u2014many amino acids determine the structure of a single protein, many genes determine the fate of a cell, many neurons are involved in shaping our thoughts and memories. Physicists have long hoped that these collective behaviors could be described using the ideas and methods of statistical mechanics. In the past few years, new, larger scale experiments have made it possible to construct statistical mechanics models of biological systems directly from real data. We review the surprising successes of this \u201cinverse\u201d approach, using examples from families of proteins, networks of neurons, and flocks of birds. Remarkably, in all these cases the models that emerge from the data are poised near a very special point in their parameter space\u2014a critical point. This suggests there may be some deeper theoretical principle \u2026",
            "Are biological systems poised at criticality?",
            "Thierry Mora and William Bialek",
            "2011",
            "fQ-BSlgAAAAJ:2P1L_qKh6hAC",
            583,
            "https:\/\/link.springer.com\/article\/10.1007\/s10955-011-0229-4",
            "5980248806712103287",
            "\/scholar?cites=5980248806712103287",
            {
                "2011":7,
                "2012":28,
                "2013":33,
                "2014":41,
                "2015":60,
                "2016":53,
                "2017":86,
                "2018":80,
                "2019":90,
                "2020":92,
                "2021":6
            }
        ]
    ]
}
{
    "columns":[
        "abstract",
        "title",
        "author",
        "pub_year",
        "author_pub_id",
        "num_citations",
        "pub_url",
        "cites_id",
        "citedby_url",
        "cites_per_year"
    ],
    "index":[
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
    ],
    "data":[
        [
            "Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a mid-level of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events (event2event) and the generation of natural language sentences from events (event2sentence). We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language.",
            "Event representations for automated story generation with deep neural nets",
            "Lara Martin and Prithviraj Ammanabrolu and Xinyu Wang and William Hancock and Shruti Singh and Brent Harrison and Mark Riedl",
            "2018",
            "JUAIpDcAAAAJ:d1gkVwhDpl0C",
            113,
            "https:\/\/ojs.aaai.org\/index.php\/AAAI\/article\/view\/11430",
            "9614219931725711912",
            "\/scholar?cites=9614219931725711912",
            {
                "2017":4,
                "2018":23,
                "2019":40,
                "2020":46
            }
        ],
        [
            "Multiplayer Online Battle Arena (MOBA) games rely primarily on combat to determine the ultimate outcome of the game. Combat in these types of games is highly-dynamic and can be difficult for novice players to learn. Typically, mastery of combat requires that players obtain expert knowledge through practice, which can be difficult to concisely describe. In this paper, we present a data-driven approach for discovering patterns in combat tactics that are common among winning teams in MOBA games. We model combat as a sequence of graphs and extract patterns that predict successful outcomes not just of combat, but of the entire game. To identify those patterns, we attribute features to these graphs using well known graph metrics. These features allow us to describe, in meaningful terms, how different combat tactics contribute to team success. We also present an evaluation of our methodology on the popular MOBA game, DotA 2 (Defense of the Ancients 2). Experiments show that extracted patterns achieve an 80% prediction accuracy when testing on new game logs.",
            "Identifying patterns in combat that are predictive of success in MOBA games.",
            "Pu Yang and Brent E Harrison and David L Roberts",
            "2014",
            "JUAIpDcAAAAJ:4DMP91E08xMC",
            101,
            "https:\/\/people.engr.ncsu.edu\/dlrober4\/papers\/fdg14-combat.pdf",
            "16351161011272106467",
            "\/scholar?cites=16351161011272106467",
            {
                "2014":3,
                "2015":8,
                "2016":11,
                "2017":20,
                "2018":23,
                "2019":20,
                "2020":14
            }
        ],
        [
            "Value alignment is a property of an intelligent agent indicating that it can only pursue goals that are beneficial to humans. Successful value alignment should ensure that an artificial general intelligence cannot intentionally or unintentionally perform behaviors that adversely affect humans. This is problematic in practice since it is difficult to exhaustively enumerated by human programmers. In order for successful value alignment, we argue that values should be learned. In this paper, we hypothesize that an artificial intelligence that can read and understand stories can learn the values tacitly held by the culture from which the stories originate. We describe preliminary work on using stories to generate a value-aligned reward signal for reinforcement learning agents that prevents psychotic-appearing behavior.",
            "Using Stories to Teach Human Values to Artificial Agents.",
            "Mark O Riedl and Brent Harrison",
            "2016",
            "JUAIpDcAAAAJ:roLk4NBRz8UC",
            68,
            "https:\/\/kevincorbett.com\/wp-content\/uploads\/2016\/04\/Using-Stories-to-Teach-Human-Values-to-Artificial-Agents.pdf",
            "8021199907582390896",
            "\/scholar?cites=8021199907582390896",
            {
                "2016":8,
                "2017":14,
                "2018":21,
                "2019":12,
                "2020":10
            }
        ],
        [
            "Games with a strong notion of story are increasingly popular. With the increased amount of story content associated with games where player decisions significantly change the course of the game (branching games), comes an increase in the effort required to author those games. Despite the increased popularity of these kinds of games, it is unclear if a typical player is able to appreciate the rich content of these games, since any given player typically only experiences a small amount of that content. We create a non-branching game that simulates branching choices by providing players with choices followed by immediate textual feedback. We hypothesize that this game, where player decisions do not significantly change the course of the game, will maintain the player\u2019s sense of agency. Experimentation showed that in a text-based story with forced-choice points there were in most cases no significant \u2026",
            "Achieving the illusion of agency",
            "Matthew William Fendt and Brent Harrison and Stephen G Ware and Rogelio E Cardona-Rivera and David L Roberts",
            "2012",
            "JUAIpDcAAAAJ:9yKSN-GCB0IC",
            59,
            "https:\/\/link.springer.com\/chapter\/10.1007\/978-3-642-34851-8_11",
            "1593666856278383947",
            "\/scholar?cites=1593666856278383947",
            {
                "2013":4,
                "2014":5,
                "2015":7,
                "2016":6,
                "2017":7,
                "2018":15,
                "2019":7,
                "2020":8
            }
        ],
        [
            "In order for robots to learn from people with no machine learning expertise, robots should learn from natural human instruction. Most machine learning techniques that incorporate explanations require people to use a limited vocabulary and provide state information, even if it is not intuitive. This paper discusses a software agent that learned to play the Mario Bros. game using explanations. Our goals to improve learning from explanations were twofold: (1) to filter explanations into advice and warnings and (2) to learn policies from sentences without state information. We used sentiment analysis to filter explanations into advice of what to do and warnings of what to avoid. We developed object-focused advice to represent what actions the agent should take when dealing with objects. A reinforcement learning agent used object-focused advice to learn policies that maximized its reward. After mitigating false negatives \u2026",
            "Learning from explanations using sentiment and advice in RL",
            "Samantha Krening and Brent Harrison and Karen M Feigh and Charles Lee Isbell and Mark Riedl and Andrea Thomaz",
            "2016",
            "JUAIpDcAAAAJ:LkGwnXOMwfcC",
            56,
            "https:\/\/ieeexplore.ieee.org\/abstract\/document\/7742965\/",
            "10651357971121844174",
            "\/scholar?cites=10651357971121844174",
            {
                "2017":4,
                "2018":18,
                "2019":15,
                "2020":18,
                "2021":1
            }
        ],
        [
            "In this paper, we present a data-driven technique for designing models of user behavior. Previously, player models were designed using user surveys, small-scale observation experiments, or knowledge engineering. These methods generally produced semantically meaningful models that were limited in their applicability. To address this, we have developed a purely data-driven methodology for generating player models based on past observations of other players. Our underlying assumption is that we can accurately predict what a player will do in a given situation if we examine enough data from former players that were in similar situations. We have chosen to test our method on achievement data from the MMORPG World of Warcraft. Experiments show that our method greatly outperforms a baseline algorithm in both precision and recall, proving that this method can create accurate player models based solely \u2026",
            "Using sequential observations to model and predict player behavior",
            "Brent Harrison and David L Roberts",
            "2011",
            "JUAIpDcAAAAJ:UeHWp8X0CEIC",
            54,
            "https:\/\/dl.acm.org\/doi\/abs\/10.1145\/2159365.2159378",
            "3375011970286087369",
            "\/scholar?cites=3375011970286087369",
            {
                "2011":1,
                "2012":7,
                "2013":7,
                "2014":7,
                "2015":2,
                "2016":7,
                "2017":7,
                "2018":4,
                "2019":7,
                "2020":5
            }
        ],
        [
            "Conflict is an essential element of interesting stories. In this paper, we operationalize a narratological definition of conflict and extend established narrative planning techniques to incorporate this definition. The conflict partial order causal link planning algorithm (CPOCL) allows narrative conflict to arise in a plan while maintaining causal soundness and character believability. We also define seven dimensions of conflict in terms of this algorithm's knowledge representation. The first three-participants, reason, and duration-are discrete values which answer the \u201cwho?\u201d \u201cwhy?\u201d and \u201cwhen?\u201d questions, respectively. The last four-balance, directness, stakes, and resolution-are continuous values which describe important narrative properties that can be used to select conflicts based on the author's purpose. We also present the results of two empirical studies which validate our operationalizations of these narrative \u2026",
            "A computational model of plan-based narrative conflict at the fabula level",
            "Stephen G Ware and R Michael Young and Brent Harrison and David L Roberts",
            "2013",
            "JUAIpDcAAAAJ:Wp0gIr-vW9MC",
            53,
            "https:\/\/ieeexplore.ieee.org\/abstract\/document\/6623111\/",
            "14079943636679093273",
            "\/scholar?cites=14079943636679093273",
            {
                "2013":1,
                "2014":3,
                "2015":5,
                "2016":14,
                "2017":8,
                "2018":10,
                "2019":5,
                "2020":7
            }
        ],
        [
            "We introduce\\em AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had performed the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of an autonomous agent into natural language. We evaluate our technique in the Frogger game environment, training an autonomous game playing agent to rationalize its action choices using natural language. A natural language training corpus is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation and show the results of two experiments evaluating the effectiveness of rationalization. Results of these evaluations show that neural machine translation is able to accurately generate rationalizations that describe agent behavior, and that \u2026",
            "Rationalization: A neural machine translation approach to generating natural language explanations",
            "Upol Ehsan and Brent Harrison and Larry Chan and Mark O Riedl",
            "2018",
            "JUAIpDcAAAAJ:2osOgNQ5qMEC",
            51,
            "https:\/\/dl.acm.org\/doi\/abs\/10.1145\/3278721.3278736",
            "1029752019269791424",
            "\/scholar?cites=1029752019269791424",
            {
                "2018":6,
                "2019":17,
                "2020":26,
                "2021":1
            }
        ],
        [
            "Automated rationale generation is an approach for real-time explanation generation whereby a computational model learns to translate an autonomous agent's internal state and action data representations into natural language. Training on human explanation data can enable agents to learn to generate human-like explanations for their behavior. In this paper, using the context of an agent that plays Frogger, we describe (a) how to collect a corpus of explanations,(b) how to train a neural rationale generator to produce different styles of rationales, and (c) how people perceive these rationales. We conducted two user studies. The first study establishes the plausibility of each type of generated rationale and situates their user perceptions along the dimensions of confidence, humanlike-ness, adequate justification, and understandability. The second study further explores user preferences between the generated \u2026",
            "Automated rationale generation: a technique for explainable AI and its effects on human perceptions",
            "Upol Ehsan and Pradyumna Tambwekar and Larry Chan and Brent Harrison and Mark O Riedl",
            "2019",
            "JUAIpDcAAAAJ:IWHjjKOFINEC",
            46,
            "https:\/\/dl.acm.org\/doi\/abs\/10.1145\/3301275.3302316",
            "1942138655641833415",
            "\/scholar?cites=1942138655641833415",
            {
                "2019":10,
                "2020":31,
                "2021":4
            }
        ],
        [
            "Games are often designed to shape player behavior in a desired way; however, it can be unclear how design decisions affect the space of behaviors in a game. Designers usually explore this space through human playtesting, which can be time-consuming and of limited effectiveness in exhausting the space of possible behaviors. In this paper, we propose the use of automated planning agents to simulate humans of varying skill levels to generate game playthroughs. Metrics can then be gathered from these playthroughs to evaluate the current game design and identify its potential flaws. We demonstrate this technique in two games: the popular word game Scrabble and a collectible card game of our own design named Cardonomicon. Using these case studies, we show how using simulated agents to model humans of varying skill levels allows us to extract metrics to describe game balance (in the case of Scrabble) and highlight potential design flaws (in the case of Cardonomicon).",
            "Monte-carlo tree search for simulation-based strategy analysis",
            "Alexander Zook and Brent Harrison and Mark O Riedl",
            "2019",
            "JUAIpDcAAAAJ:UebtZRa9Y70C",
            36,
            "https:\/\/arxiv.org\/abs\/1908.01423",
            "6202555024123403704",
            "\/scholar?cites=6202555024123403704",
            {
                "2015":1,
                "2016":4,
                "2017":3,
                "2018":12,
                "2019":10,
                "2020":6
            }
        ]
    ]
}
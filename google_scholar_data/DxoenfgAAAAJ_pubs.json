{
    "columns":[
        "abstract",
        "title",
        "author",
        "pub_year",
        "author_pub_id",
        "num_citations",
        "pub_url",
        "cites_id",
        "citedby_url",
        "cites_per_year"
    ],
    "index":[
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
    ],
    "data":[
        [
            "We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.",
            "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms",
            "Michael Collins",
            "2002",
            "DxoenfgAAAAJ:qjMakFHDy7sC",
            2545,
            "https:\/\/www.aclweb.org\/anthology\/W02-1001.pdf",
            "13138599272647465483",
            "\/scholar?cites=13138599272647465483",
            {
                "2002":7,
                "2003":15,
                "2004":45,
                "2005":81,
                "2006":83,
                "2007":123,
                "2008":116,
                "2009":170,
                "2010":183,
                "2011":186,
                "2012":163,
                "2013":208,
                "2014":238,
                "2015":230,
                "2016":198,
                "2017":167,
                "2018":134,
                "2019":97,
                "2020":71,
                "2021":5
            }
        ],
        [
            "This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of \u2026",
            "Head-driven statistical models for natural language parsing",
            "Michael Collins",
            "2003",
            "DxoenfgAAAAJ:9yKSN-GCB0IC",
            2458,
            "https:\/\/www.mitpressjournals.org\/doi\/abs\/10.1162\/089120103322753356",
            "15742121240912269262",
            "\/scholar?cites=15742121240912269262",
            {
                "2002":46,
                "2003":108,
                "2004":134,
                "2005":176,
                "2006":180,
                "2007":196,
                "2008":187,
                "2009":192,
                "2010":220,
                "2011":147,
                "2012":124,
                "2013":139,
                "2014":107,
                "2015":104,
                "2016":84,
                "2017":74,
                "2018":53,
                "2019":44,
                "2020":40,
                "2021":1
            }
        ],
        [
            "In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1\/87.5% constituent precision\/recall, an average improvement of 2.3% over (Collins 96).",
            "Three generative, lexicalised models for statistical parsing",
            "Michael Collins",
            "1997",
            "DxoenfgAAAAJ:2osOgNQ5qMEC",
            1357,
            "https:\/\/arxiv.org\/abs\/cmp-lg\/9706022",
            "3052991574894636359",
            "\/scholar?cites=3052991574894636359",
            {
                "1997":8,
                "1998":28,
                "1999":35,
                "2000":70,
                "2001":50,
                "2002":84,
                "2003":82,
                "2004":98,
                "2005":96,
                "2006":91,
                "2007":82,
                "2008":83,
                "2009":67,
                "2010":61,
                "2011":53,
                "2012":63,
                "2013":49,
                "2014":48,
                "2015":35,
                "2016":37,
                "2017":36,
                "2018":23,
                "2019":22,
                "2020":17,
                "2021":1
            }
        ],
        [
            "This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier. However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple\" seed\" rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context inwhich it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).",
            "Unsupervised models for named entity classification",
            "Michael Collins and Yoram Singer",
            "1999",
            "DxoenfgAAAAJ:IjCSPb-OGe4C",
            1190,
            "https:\/\/www.aclweb.org\/anthology\/W99-0613.pdf",
            "7758805788386282720",
            "\/scholar?cites=7758805788386282720",
            {
                "2000":22,
                "2001":26,
                "2002":54,
                "2003":63,
                "2004":60,
                "2005":68,
                "2006":64,
                "2007":61,
                "2008":73,
                "2009":79,
                "2010":85,
                "2011":70,
                "2012":56,
                "2013":80,
                "2014":47,
                "2015":56,
                "2016":45,
                "2017":32,
                "2018":38,
                "2019":58,
                "2020":26,
                "2021":2
            }
        ],
        [
            "We describe the application of kernel methods to Natural Language Processing (NLP) problems. In many NLP tasks the objects being modeled are strings, trees, graphs or other discrete structures which require some mechanism to convert them into feature vectors. We describe kernels for various natural language structures, allowing rich, high dimensional representations of these structures. We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and we give experimental results on the ATIS corpus of parse trees.",
            "Convolution kernels for natural language",
            "Michael Collins and Nigel Duffy",
            "2001",
            "DxoenfgAAAAJ:zYLM7Y9cAGgC",
            1166,
            "http:\/\/www-2.cs.cmu.edu\/Groups\/NIPS\/NIPS2001\/papers\/psgz\/AA58.ps.gz",
            "9481191489485235027",
            "\/scholar?cites=9481191489485235027",
            {
                "2002":10,
                "2003":42,
                "2004":38,
                "2005":66,
                "2006":53,
                "2007":75,
                "2008":73,
                "2009":74,
                "2010":90,
                "2011":96,
                "2012":73,
                "2013":71,
                "2014":67,
                "2015":70,
                "2016":59,
                "2017":52,
                "2018":62,
                "2019":40,
                "2020":29,
                "2021":1
            }
        ],
        [
            "We present a discriminative latent variable model for classification problems in structured domains where inputs can be represented by a graph of local observations. A hidden-state conditional random field framework learns a set of latent variables conditioned on local features. Observations need not be independent and may overlap in space and time.",
            "Hidden conditional random fields",
            "Ariadna Quattoni and Sybor Wang and Louis-Philippe Morency and Morency Collins and Trevor Darrell",
            "2007",
            "DxoenfgAAAAJ:ufrVoPGSRksC",
            1163,
            "https:\/\/ieeexplore.ieee.org\/abstract\/document\/4293212\/",
            "11754671731819159561",
            "\/scholar?cites=11754671731819159561",
            {
                "2004":3,
                "2005":13,
                "2006":39,
                "2007":56,
                "2008":56,
                "2009":55,
                "2010":74,
                "2011":94,
                "2012":98,
                "2013":111,
                "2014":95,
                "2015":121,
                "2016":104,
                "2017":83,
                "2018":63,
                "2019":49,
                "2020":27
            }
        ],
        [
            "This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95, Jelinek et al 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.",
            "A new statistical parser based on bigram lexical dependencies",
            "Michael Collins",
            "1996",
            "DxoenfgAAAAJ:UeHWp8X0CEIC",
            924,
            "https:\/\/arxiv.org\/abs\/cmp-lg\/9605012",
            "2616612216294260743",
            "\/scholar?cites=2616612216294260743",
            {
                "1996":8,
                "1997":56,
                "1998":41,
                "1999":51,
                "2000":60,
                "2001":41,
                "2002":54,
                "2003":63,
                "2004":61,
                "2005":68,
                "2006":49,
                "2007":53,
                "2008":29,
                "2009":38,
                "2010":42,
                "2011":33,
                "2012":24,
                "2013":16,
                "2014":16,
                "2015":26,
                "2016":19,
                "2017":10,
                "2018":8,
                "2019":18,
                "2020":10
            }
        ],
        [
            "This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins \u2026",
            "Discriminative reranking for natural language parsing",
            "Michael Collins and Terry Koo",
            "2005",
            "DxoenfgAAAAJ:Tyk-4Ss8FVUC",
            898,
            "https:\/\/www.mitpressjournals.org\/doi\/abs\/10.1162\/0891201053630273",
            "11945992233064279861",
            "\/scholar?cites=11945992233064279861",
            {
                "2000":5,
                "2001":22,
                "2002":27,
                "2003":32,
                "2004":30,
                "2005":58,
                "2006":61,
                "2007":54,
                "2008":73,
                "2009":55,
                "2010":69,
                "2011":63,
                "2012":62,
                "2013":60,
                "2014":54,
                "2015":41,
                "2016":32,
                "2017":36,
                "2018":14,
                "2019":25,
                "2020":12
            }
        ],
        [
            "This paper addresses the problem of mapping natural language sentences to lambda-calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains.",
            "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
            "Luke S Zettlemoyer and Michael Collins",
            "2012",
            "DxoenfgAAAAJ:0EnyYjriUFMC",
            824,
            "https:\/\/arxiv.org\/abs\/1207.1420",
            "12511188869985993063",
            "\/scholar?cites=12511188869985993063",
            {
                "2005":6,
                "2006":14,
                "2007":20,
                "2008":22,
                "2009":28,
                "2010":25,
                "2011":36,
                "2012":39,
                "2013":53,
                "2014":72,
                "2015":73,
                "2016":63,
                "2017":79,
                "2018":97,
                "2019":103,
                "2020":80,
                "2021":5
            }
        ],
        [
            "We give a unified account of boosting and logistic regression in which each learning problem is cast in terms of optimization of Bregman distances. The striking similarity of the two problems in this framework allows us to design and analyze algorithms for both simultaneously, and to easily adapt algorithms designed for one problem to the other. For both problems, we give new algorithms and explain their potential advantages over existing methods. These algorithms are iterative and can be divided into two types based on whether the parameters are updated sequentially (one at a time) or in parallel (all at once). We also describe a parameterized family of algorithms that includes both a sequential- and a parallel-update algorithm as special cases, thus showing how the sequential and parallel approaches can themselves be unified. For all of the algorithms, we give convergence proofs using a general \u2026",
            "Logistic regression, AdaBoost and Bregman distances",
            "Michael Collins and Robert E Schapire and Yoram Singer",
            "2002",
            "DxoenfgAAAAJ:Y0pCki6q_DkC",
            805,
            "https:\/\/link.springer.com\/article\/10.1023\/A:1013912006537",
            "8402109090401422201",
            "\/scholar?cites=8402109090401422201",
            {
                "2001":10,
                "2002":28,
                "2003":29,
                "2004":29,
                "2005":35,
                "2006":44,
                "2007":50,
                "2008":53,
                "2009":37,
                "2010":43,
                "2011":38,
                "2012":46,
                "2013":38,
                "2014":40,
                "2015":39,
                "2016":45,
                "2017":49,
                "2018":40,
                "2019":47,
                "2020":45,
                "2021":2
            }
        ]
    ]
}
{
    "columns":[
        "abstract",
        "title",
        "author",
        "pub_year",
        "author_pub_id",
        "num_citations",
        "pub_url",
        "cites_id",
        "citedby_url",
        "cites_per_year"
    ],
    "index":[
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
    ],
    "data":[
        [
            "What does it mean to say that a certain set of spikes is the right answer to a computational problem? In what sense does a spike train convey information about the sensory world? Spikes begins by providing precise formulations of these and related questions about the representation of sensory signals in neural spike trains. The answers to these questions are then pursued in experiments on sensory neurons. Intended for neurobiologists with an interest in mathematical analysis of neural data as well as the growing number of physicists and mathematicians interested in information processing by\" real\" nervous systems, Spikes provides a self-contained review of relevant concepts in information theory and statistical decision theory. Our perception of the world is driven by input from the sensory nerves. This input arrives encoded as sequences of identical spikes. Much of neural computation involves processing these spike trains. What does it mean to say that a certain set of spikes is the right answer to a computational problem? In what sense does a spike train convey information about the sensory world? Spikes begins by providing precise formulations of these and related questions about the representation of sensory signals in neural spike trains. The answers to these questions are then pursued in experiments on sensory neurons. The authors invite the reader to play the role of a hypothetical observer inside the brain who makes decisions based on the incoming spike trains. Rather than asking how a neuron responds to a given stimulus, the authors ask how the brain could make inferences about an unknown stimulus from a given neural \u2026",
            "Spikes: exploring the neural code",
            "Fred Rieke and David Warland",
            "1999",
            "mT0AyyIAAAAJ:4JMBOYKVnBMC",
            3418,
            "http:\/\/scholar.google.com\/scholar?cluster=9090819478669086053&hl=en&oi=scholarr",
            "9090819478669086053",
            "\/scholar?cites=9090819478669086053",
            {
                "1998":60,
                "1999":81,
                "2000":99,
                "2001":152,
                "2002":154,
                "2003":151,
                "2004":156,
                "2005":143,
                "2006":174,
                "2007":198,
                "2008":186,
                "2009":174,
                "2010":179,
                "2011":204,
                "2012":160,
                "2013":191,
                "2014":135,
                "2015":134,
                "2016":119,
                "2017":145,
                "2018":115,
                "2019":113,
                "2020":106,
                "2021":10
            }
        ],
        [
            "Traditional approaches to neural coding characterize the encoding of known stimuli in average neural responses. Organisms face nearly the opposite task--extracting information about an unknown time-dependent stimulus from short segments of a spike train. Here the neural code was characterized from the point of view of the organism, culminating in algorithms for real-time stimulus estimation based on a single example of the spike train. These methods were applied to an identified movement-sensitive neuron in the fly visual system. Such decoding experiments determined the effective noise level and fault tolerance of neural computation, and the structure of the decoding algorithms suggested a simple model for real-time analog signal processing with spiking neurons.",
            "Reading a neural code",
            "William Bialek and Fred Rieke and RR De Ruyter Van Steveninck and David Warland",
            "1991",
            "mT0AyyIAAAAJ:lSLTfruPkqcC",
            1213,
            "https:\/\/science.sciencemag.org\/content\/252\/5014\/1854.abstract",
            "9888408790689901978",
            "\/scholar?cites=9888408790689901978",
            {
                "1991":6,
                "1992":13,
                "1993":15,
                "1994":18,
                "1995":19,
                "1996":43,
                "1997":37,
                "1998":34,
                "1999":39,
                "2000":29,
                "2001":51,
                "2002":52,
                "2003":25,
                "2004":47,
                "2005":52,
                "2006":38,
                "2007":40,
                "2008":66,
                "2009":31,
                "2010":48,
                "2011":58,
                "2012":57,
                "2013":62,
                "2014":35,
                "2015":45,
                "2016":47,
                "2017":57,
                "2018":39,
                "2019":49,
                "2020":35,
                "2021":1
            }
        ],
        [
            "The nervous system represents time dependent signals in sequences of discrete, identical action potentials or spikes; information is carried only in the spike arrival times. We show how to quantify this information, in bits, free from any assumptions about which features of the spike train or input signal are most important, and we apply this approach to the analysis of experiments on a motion sensitive neuron in the fly visual system. This neuron transmits information about the visual stimulus at rates of up to 90 bits\/s, within a factor of 2 of the physical limit set by the entropy of the spike train itself.",
            "Entropy and information in neural spike trains",
            "Steven P Strong and Roland Koberle and Rob R De Ruyter Van Steveninck and William Bialek",
            "1998",
            "mT0AyyIAAAAJ:-f6ydRqryjwC",
            957,
            "https:\/\/journals.aps.org\/prl\/abstract\/10.1103\/PhysRevLett.80.197",
            "18030712511737164673",
            "\/scholar?cites=18030712511737164673",
            {
                "1999":14,
                "2000":21,
                "2001":40,
                "2002":23,
                "2003":38,
                "2004":39,
                "2005":35,
                "2006":41,
                "2007":43,
                "2008":56,
                "2009":33,
                "2010":55,
                "2011":58,
                "2012":55,
                "2013":53,
                "2014":35,
                "2015":54,
                "2016":49,
                "2017":51,
                "2018":48,
                "2019":61,
                "2020":39,
                "2021":2
            }
        ],
        [
            "We examine the dynamics of a neural code in the context of stimuli whose statistical properties are themselves evolving dynamically. Adaptation to these statistics occurs over a wide range of timescales\u2014from tens of milliseconds to minutes. Rapid components of adaptation serve to optimize the information that action potentials carry about rapid stimulus variations within the local statistical ensemble, while changes in the rate and statistics of action-potential firing encode information about the ensemble itself, thus resolving potential ambiguities. The speed with which information is optimized and ambiguities are resolved approaches the physical limit imposed by statistical sampling and noise.",
            "Efficiency and ambiguity in an adaptive neural code",
            "Adrienne L Fairhall and Geoffrey D Lewen and William Bialek and Robert R de Ruyter van Steveninck",
            "2001",
            "mT0AyyIAAAAJ:isC4tDSrTZIC",
            804,
            "https:\/\/www.nature.com\/articles\/35090500",
            "16382135857568142103",
            "\/scholar?cites=16382135857568142103",
            {
                "2001":6,
                "2002":20,
                "2003":31,
                "2004":30,
                "2005":35,
                "2006":38,
                "2007":55,
                "2008":36,
                "2009":30,
                "2010":42,
                "2011":33,
                "2012":56,
                "2013":48,
                "2014":42,
                "2015":57,
                "2016":53,
                "2017":43,
                "2018":44,
                "2019":50,
                "2020":47
            }
        ],
        [
            "We derive experimentally based estimates of the energy used by neural mechanisms to code known quantities of information. Biophysical measurements from cells in the blowfly retina yield estimates of the ATP required to generate graded (analog) electrical signals that transmit known amounts of information. Energy consumption is several orders of magnitude greater than the thermodynamic minimum. It costs 10 4 ATP molecules to transmit a bit at a chemical synapse, and 10 6-10 7 ATP for graded signals in an interneuron or a photoreceptor, or for spike coding. Therefore, in noise-limited signaling systems, a weak pathway of low capacity transmits information more economically, which promotes the distribution of information among multiple pathways.",
            "The metabolic cost of neural information",
            "Simon B Laughlin and Rob R de Ruyter van Steveninck and John C Anderson",
            "1998",
            "mT0AyyIAAAAJ:WF5omc3nYNoC",
            774,
            "https:\/\/www.nature.com\/articles\/nn0598_36",
            "5180617447733767483",
            "\/scholar?cites=5180617447733767483",
            {
                "1999":9,
                "2000":14,
                "2001":39,
                "2002":22,
                "2003":21,
                "2004":19,
                "2005":11,
                "2006":31,
                "2007":25,
                "2008":35,
                "2009":33,
                "2010":18,
                "2011":32,
                "2012":45,
                "2013":50,
                "2014":43,
                "2015":47,
                "2016":57,
                "2017":61,
                "2018":48,
                "2019":49,
                "2020":54,
                "2021":5
            }
        ],
        [
            "Adaptation is a widespread phenomenon in nervous systems, providing flexibility to function under varying external conditions. Here, we relate an adaptive property of a sensory system directly to its function as a carrier of information about input signals. We show that the input\/output relation of a sensory system in a dynamic environment changes with the statistical properties of the environment. Specifically, when the dynamic range of inputs changes, the input\/output relation rescales so as to match the dynamic range of responses to that of the inputs. We give direct evidence that the scaling of the input\/output relation is set to maximize information transmission for each distribution of signals. This adaptive behavior should be particularly useful in dealing with the intermittent statistics of natural signals.",
            "Adaptive rescaling maximizes information transmission",
            "Naama Brenner and William Bialek and Rob de Ruyter Van Steveninck",
            "2000",
            "mT0AyyIAAAAJ:qjMakFHDy7sC",
            693,
            "https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0896627300812052",
            "1395484813148577473",
            "\/scholar?cites=1395484813148577473",
            {
                "2000":3,
                "2001":17,
                "2002":18,
                "2003":29,
                "2004":24,
                "2005":34,
                "2006":34,
                "2007":49,
                "2008":42,
                "2009":35,
                "2010":36,
                "2011":36,
                "2012":38,
                "2013":38,
                "2014":38,
                "2015":30,
                "2016":36,
                "2017":38,
                "2018":43,
                "2019":33,
                "2020":35
            }
        ],
        [
            "To provide information about dynamic sensory stimuli, the pattern of action potentials in spiking neurons must be variable. To ensure reliability these variations must be related, reproducibly, to the stimulus. For H1, a motion-sensitive neuron in the fly9s visual system, constant-velocity motion produces irregular spike firing patterns, and spike counts typically have a variance comparable to the mean, for cells in the mammalian cortex. But more natural, time-dependent input signals yield patterns of spikes that are much more reproducible, both in terms of timing and of counting precision. Variability and reproducibility are quantified with ideas from information theory, and measured spike sequences in H1 carry more than twice the amount of information they would if they followed the variance-mean relation seen with constant inputs. Thus, models that may accurately account for the neural response to static stimuli can \u2026",
            "Reproducibility and variability in neural spike trains",
            "Rob R de Ruyter van Steveninck and Geoffrey D Lewen and Steven P Strong and Roland Koberle and William Bialek",
            "1997",
            "mT0AyyIAAAAJ:4TOpqqG69KYC",
            556,
            "https:\/\/science.sciencemag.org\/content\/275\/5307\/1805.abstract",
            "11626131654627137375",
            "\/scholar?cites=11626131654627137375",
            {
                "1997":6,
                "1998":16,
                "1999":27,
                "2000":21,
                "2001":30,
                "2002":28,
                "2003":22,
                "2004":22,
                "2005":32,
                "2006":30,
                "2007":23,
                "2008":18,
                "2009":22,
                "2010":30,
                "2011":31,
                "2012":26,
                "2013":39,
                "2014":22,
                "2015":17,
                "2016":22,
                "2017":18,
                "2018":18,
                "2019":15,
                "2020":18,
                "2021":2
            }
        ],
        [
            "We show that the information carried by compound events in neural spike trains\u2014patterns of spikes across time or across a population of cells\u2014can be measured, independent of assumptions about what these patterns might represent. By comparing the information carried by a compound pattern with the information carried independently by its parts, we directly measure the synergy among these parts. We illustrate the use of these methods by applying them to experiments on the motion-sensitive neuron H1 of the fly's visual system, where we confirm that two spikes close together in time carry far more than twice the information carried by a single spike. We analyze the sources of this synergy and provide evidence that pairs of spikes close together in time may be especially important patterns in the code of H1.",
            "Synergy in a neural code",
            "Naama Brenner and Steven P Strong and Roland Koberle and William Bialek and Rob R de Ruyter van Steveninck",
            "2000",
            "mT0AyyIAAAAJ:dhFuZR0502QC",
            370,
            "https:\/\/www.mitpressjournals.org\/doi\/abs\/10.1162\/089976600300015259",
            "15265632766223787480",
            "\/scholar?cites=15265632766223787480",
            {
                "2000":3,
                "2001":14,
                "2002":10,
                "2003":20,
                "2004":21,
                "2005":18,
                "2006":12,
                "2007":20,
                "2008":21,
                "2009":11,
                "2010":20,
                "2011":26,
                "2012":30,
                "2013":20,
                "2014":15,
                "2015":32,
                "2016":12,
                "2017":11,
                "2018":16,
                "2019":19,
                "2020":12,
                "2021":2
            }
        ],
        [
            "Development of spatial patterns in multicellular organisms depends on gradients in the concentration of signaling molecules that control gene expression. In the Drosophila embryo, Bicoid (Bcd) morphogen controls cell fate along 70% of the anteroposterior axis but is translated from mRNA localized at the anterior pole. Gradients of Bcd and other morphogens are thought to arise through diffusion, but this basic assumption has never been rigorously tested in living embryos. Furthermore, because diffusion sets a relationship between length and time scales, it is hard to see how patterns of gene expression established by diffusion would scale proportionately as egg size changes during evolution. Here, we show that the motion of inert molecules through the embryo is well described by the diffusion equation on the relevant length and time scales, and that effective diffusion constants are essentially the same in closely \u2026",
            "Diffusion and scaling during early embryonic pattern formation",
            "Thomas Gregor and William Bialek and Rob R De Ruyter Van Steveninck and David W Tank and Eric F Wieschaus",
            "2005",
            "mT0AyyIAAAAJ:MXK_kJrjxJIC",
            328,
            "https:\/\/www.pnas.org\/content\/102\/51\/18403.short",
            "10778391349123149745",
            "\/scholar?cites=10778391349123149745",
            {
                "2006":9,
                "2007":20,
                "2008":21,
                "2009":23,
                "2010":29,
                "2011":35,
                "2012":14,
                "2013":36,
                "2014":22,
                "2015":26,
                "2016":18,
                "2017":17,
                "2018":20,
                "2019":16,
                "2020":18,
                "2021":2
            }
        ],
        [
            "The major problem in information theoretic analysis of neural responses and other biological data is the reliable estimation of entropy-like quantities from small samples. We apply a recently introduced Bayesian entropy estimator to synthetic data inspired by experiments, and to real experimental spike trains. The estimator performs admirably even very deep in the undersampled regime, where other techniques fail. This opens new possibilities for the information theoretic analysis of experiments, and may be of general interest as an example of learning from limited data.",
            "Entropy and information in neural spike trains: Progress on the sampling problem",
            "Ilya Nemenman and William Bialek and Rob De Ruyter Van Steveninck",
            "2004",
            "mT0AyyIAAAAJ:2osOgNQ5qMEC",
            284,
            "https:\/\/journals.aps.org\/pre\/abstract\/10.1103\/PhysRevE.69.056111",
            "1040688443121099974",
            "\/scholar?cites=1040688443121099974",
            {
                "2003":2,
                "2004":2,
                "2005":8,
                "2006":11,
                "2007":26,
                "2008":20,
                "2009":15,
                "2010":21,
                "2011":13,
                "2012":19,
                "2013":22,
                "2014":21,
                "2015":22,
                "2016":12,
                "2017":12,
                "2018":25,
                "2019":13,
                "2020":13,
                "2021":1
            }
        ]
    ]
}
{
    "columns":[
        "abstract",
        "title",
        "author",
        "pub_year",
        "author_pub_id",
        "num_citations",
        "pub_url",
        "cites_id",
        "citedby_url",
        "cites_per_year"
    ],
    "index":[
        0,
        1,
        2,
        3
    ],
    "data":[
        [
            "The report introduces a constructive learning algorithm for recurrent neural networks, which modifies only the weights to output units in order to achieve the learning task.",
            "The \u201cecho state\u201d approach to analysing and training recurrent neural networks-with an erratum note",
            "Herbert Jaeger",
            "2001",
            "0uztVbMAAAAJ:u5HHmVD_uO8C",
            2593,
            "https:\/\/www.researchgate.net\/profile\/Herbert_Jaeger3\/publication\/215385037_The_echo_state_approach_to_analysing_and_training_recurrent_neural_networks-with_an_erratum_note'\/links\/566a003508ae62b05f027be3\/The-echo-state-approach-to-analysing-and-training-recurrent-neural-networks-with-an-erratum-note.pdf",
            "11205049190723554365",
            "\/scholar?cites=11205049190723554365",
            {
                "2002":15,
                "2003":16,
                "2004":28,
                "2005":38,
                "2006":49,
                "2007":70,
                "2008":92,
                "2009":101,
                "2010":109,
                "2011":104,
                "2012":129,
                "2013":159,
                "2014":170,
                "2015":198,
                "2016":198,
                "2017":217,
                "2018":235,
                "2019":291,
                "2020":319,
                "2021":30
            }
        ],
        [
            "We present a method for learning nonlinear systems, echo state networks (ESNs). ESNs employ artificial recurrent neural networks in a way that has recently been proposed independently as a learning mechanism in biological brains. The learning method is computationally efficient and easy to use. On a benchmark task of predicting a chaotic time series, accuracy is improved by a factor of 2400 over previous techniques. The potential for engineering applications is illustrated by equalizing a communication channel, where the signal error rate is improved by two orders of magnitude.",
            "Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication",
            "Herbert Jaeger and Harald Haas",
            "2004",
            "0uztVbMAAAAJ:u-x6o8ySG0sC",
            2492,
            "https:\/\/science.sciencemag.org\/content\/304\/5667\/78.abstract",
            "11560804180973930170",
            "\/scholar?cites=11560804180973930170",
            {
                "2004":10,
                "2005":30,
                "2006":35,
                "2007":70,
                "2008":78,
                "2009":92,
                "2010":105,
                "2011":110,
                "2012":113,
                "2013":147,
                "2014":154,
                "2015":179,
                "2016":197,
                "2017":216,
                "2018":267,
                "2019":318,
                "2020":316,
                "2021":30
            }
        ],
        [
            "Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating\/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current \u201cbrand-names\u201d of reservoir methods, and thus aims to help in unifying the field and \u2026",
            "Reservoir computing approaches to recurrent neural network training",
            "Mantas Luko\u0161evi\u010dius and Herbert Jaeger",
            "2009",
            "0uztVbMAAAAJ:qjMakFHDy7sC",
            1728,
            "https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1574013709000173",
            "6340776138936674554",
            "\/scholar?cites=6340776138936674554",
            {
                "2009":5,
                "2010":43,
                "2011":69,
                "2012":102,
                "2013":121,
                "2014":123,
                "2015":119,
                "2016":175,
                "2017":191,
                "2018":205,
                "2019":237,
                "2020":286,
                "2021":30
            }
        ],
        [
            "This tutorial is a worked-out version of a 5-hour course originally held at AIS in September\/October 2002. It has two distinct components. First, it contains a mathematically-oriented crash course on traditional training methods for recurrent neural networks, covering back-propagation through time (BPTT), real-time recurrent learning (RTRL), and extended Kalman filtering approaches (EKF). This material is covered in Sections 2\u20135. The remaining sections 1 and 6\u20139 are much more gentle, more detailed, and illustrated with simple examples. They are intended to be useful as a stand-alone tutorial for the echo state network (ESN) approach to recurrent neural network training.",
            "Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the\" echo state network\" approach",
            "Herbert Jaeger",
            "2002",
            "0uztVbMAAAAJ:d1gkVwhDpl0C",
            1060,
            "https:\/\/staff.fmi.uvt.ro\/~daniela.zaharie\/am2016\/proiecte\/tehnici\/ReservoirComputing\/ESNTutorialRev.pdf",
            "2800543766969536116",
            "\/scholar?cites=2800543766969536116",
            {
                "2004":12,
                "2005":18,
                "2006":14,
                "2007":24,
                "2008":39,
                "2009":44,
                "2010":48,
                "2011":69,
                "2012":57,
                "2013":76,
                "2014":70,
                "2015":76,
                "2016":88,
                "2017":117,
                "2018":88,
                "2019":110,
                "2020":93,
                "2021":6
            }
        ]
    ]
}
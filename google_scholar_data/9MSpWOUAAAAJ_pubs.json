{
    "columns":[
        "abstract",
        "title",
        "author",
        "pub_year",
        "author_pub_id",
        "num_citations",
        "pub_url",
        "cites_id",
        "citedby_url",
        "cites_per_year"
    ],
    "index":[
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
    ],
    "data":[
        [
            "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets \u2026",
            "Imagenet: A large-scale hierarchical image database",
            "Jia Deng and Wei Dong and Richard Socher and Li-Jia Li and Kai Li and Li Fei-Fei",
            "2009",
            "9MSpWOUAAAAJ:Y0pCki6q_DkC",
            25060,
            "https:\/\/ieeexplore.ieee.org\/abstract\/document\/5206848\/",
            "610894740843277394",
            "\/scholar?cites=610894740843277394",
            {
                "2010":132,
                "2011":197,
                "2012":264,
                "2013":343,
                "2014":538,
                "2015":840,
                "2016":1397,
                "2017":2341,
                "2018":3981,
                "2019":5927,
                "2020":8150,
                "2021":554
            }
        ],
        [
            "This paper presents and characterizes the Princeton Application Repository for Shared-Memory Computers (PARSEC), a benchmark suite for studies of Chip-Multiprocessors (CMPs). Previous available benchmarks for multiprocessors have focused on high-performance computing applications and used a limited number of synchronization methods. PARSEC includes emerging applications in recognition, mining and synthesis (RMS) as well as systems applications which mimic large-scale multithreaded commercial programs. Our characterization shows that the benchmark suite covers a wide spectrum of working sets, locality, data sharing, synchronization and off-chip traffic. The benchmark suite has been made available to the public.",
            "The PARSEC benchmark suite: Characterization and architectural implications",
            "Christian Bienia and Sanjeev Kumar and Jaswinder Pal Singh and Kai Li",
            "2008",
            "9MSpWOUAAAAJ:d1gkVwhDpl0C",
            3891,
            "https:\/\/dl.acm.org\/doi\/abs\/10.1145\/1454115.1454128",
            "10965237662379785525",
            "\/scholar?cites=10965237662379785525",
            {
                "2008":45,
                "2009":122,
                "2010":230,
                "2011":281,
                "2012":306,
                "2013":324,
                "2014":353,
                "2015":404,
                "2016":412,
                "2017":373,
                "2018":331,
                "2019":316,
                "2020":264,
                "2021":6
            }
        ],
        [
            "Decentralized and unstructured peer-to-peer networks such as Gnutella are attractive for certain applications because they require no centralized directories and no precise control over network topology or data placement. However, the flooding-based query algorithm used in Gnutella does not scale; each query generates a large amount of traffic and large systems quickly become overwhelmed by the query-induced load. This paper explores, through simulation, various alternatives to Gnutella's query algorithm, data replication strategy, and network topology. We propose a query algorithm based on multiple random walks that resolves queries almost as quickly as Gnutella's flooding method while reducing the network traffic by two orders of magnitude in many cases. We also present simulation results on a distributed replication strategy proposed in [8]. Finally, we find that among the various network topologies we \u2026",
            "Search and replication in unstructured peer-to-peer networks",
            "Qin Lv and Pei Cao and Edith Cohen and Kai Li and Scott Shenker",
            "2002",
            "9MSpWOUAAAAJ:u5HHmVD_uO8C",
            2683,
            "https:\/\/dl.acm.org\/doi\/abs\/10.1145\/514191.514206",
            "17818466769784722704",
            "\/scholar?cites=17818466769784722704",
            {
                "2001":9,
                "2002":27,
                "2003":100,
                "2004":186,
                "2005":219,
                "2006":278,
                "2007":314,
                "2008":274,
                "2009":242,
                "2010":240,
                "2011":157,
                "2012":144,
                "2013":114,
                "2014":76,
                "2015":55,
                "2016":54,
                "2017":37,
                "2018":38,
                "2019":31,
                "2020":28
            }
        ],
        [
            "The memory coherence problem in designing and implementing a shared virtual memory on loosely coupled multiprocessors is studied in depth. Two classes of algorithms, centralized and distributed, for solving the problem are presented. A prototype shared virtual memory on an Apollo ring based on these algorithms has been implemented. Both theoretical and practical results show that the memory coherence problem can indeed be solved efficiently on a loosely coupled multiprocessor.",
            "Memory coherence in shared virtual memory systems",
            "Kai Li and Paul Hudak",
            "1989",
            "9MSpWOUAAAAJ:u-x6o8ySG0sC",
            2075,
            "https:\/\/dl.acm.org\/doi\/abs\/10.1145\/75104.75105",
            "6112614815415691041",
            "\/scholar?cites=6112614815415691041",
            {
                "1988":14,
                "1989":21,
                "1990":45,
                "1991":62,
                "1992":82,
                "1993":130,
                "1994":124,
                "1995":125,
                "1996":118,
                "1997":110,
                "1998":119,
                "1999":103,
                "2000":97,
                "2001":73,
                "2002":54,
                "2003":69,
                "2004":69,
                "2005":48,
                "2006":43,
                "2007":65,
                "2008":33,
                "2009":61,
                "2010":29,
                "2011":39,
                "2012":39,
                "2013":41,
                "2014":45,
                "2015":35,
                "2016":31,
                "2017":27,
                "2018":28,
                "2019":26,
                "2020":21,
                "2021":3
            }
        ],
        [
            "Benchmarking has become one of the most important methods for quantitative performance evaluation of processor and computer system designs. Benchmarking of modern multiprocessors such as chip multiprocessors is challenging because of their application domain, scalability and parallelism requirements. In my thesis, I have developed a methodology to design effective benchmark suites and demonstrated its effectiveness by developing and deploying a benchmark suite for evaluating multiprocessors. More specifically, this thesis includes several contributions. First, the thesis shows that a new benchmark suite for multiprocessors is needed because the behavior of modern parallel programs is significantly different from those represented by SPLASH-2, the most popular parallel benchmark suite developed over ten years ago. Second, the thesis quantitatively describes the requirements and characteristics of a set of multithreaded programs and their underlying technology trends. Third, the thesis presents a systematic approach to scale and select benchmark inputs with the goal of optimizing benchmarking accuracy subject to constrained execution or simulation time. Finally, the thesis describes a parallel benchmark suite called PARSEC for evaluating modern shared-memory multiprocessors. Since its initial release, PARSEC has been adopted by many architecture groups in both research and industry. iii",
            "Benchmarking modern multiprocessors",
            "Christian Bienia and Kai Li",
            "2011",
            "9MSpWOUAAAAJ:LPtt_HFRSbwC",
            1291,
            "ftp:\/\/ftp.cs.princeton.edu\/techreports\/2010\/890.pdf",
            "2002666508022924647",
            "\/scholar?cites=2002666508022924647",
            {
                "2011":34,
                "2012":95,
                "2013":129,
                "2014":153,
                "2015":168,
                "2016":158,
                "2017":133,
                "2018":107,
                "2019":137,
                "2020":122,
                "2021":3
            }
        ],
        [
            "Disk-based deduplication storage has emerged as the new-generation storage system for enterprise data protection to replace tape libraries. Deduplication removes redundant data segments to compress data into a highly compact form and makes it economical to store backups on disk instead of tape. A crucial requirement for enterprise data protection is high throughput, typically over 100 MB\/sec, which enables backups to complete quickly. A significant challenge is to identify and eliminate duplicate data segments at this rate on a low-cost system that cannot afford enough RAM to store an index of the stored segments and may be forced to access an on-disk index for every input segment.This paper describes three techniques employed in the production Data Domain deduplication file system to relieve the disk bottleneck. These techniques include:(1) the Summary Vector, a compact in-memory data structure for identifying new segments;(2) Stream-Informed Segment Layout, a data layout method to improve on-disk locality for sequentially accessed segments; and (3) Locality Preserved Caching, which maintains the locality of the fingerprints of duplicate segments to achieve high cache hit ratios. Together, they can remove 99% of the disk accesses for deduplication of real world workloads. These techniques enable a modern two-socket dual-core system to run at 90% CPU utilization with only one shelf of 15 disks and achieve 100 MB\/sec for single-stream throughput and 210 MB\/sec for multi-stream throughput.",
            "Avoiding the disk bottleneck in the data domain deduplication file system.",
            "Benjamin Zhu and Kai Li and R Hugo Patterson",
            "2008",
            "9MSpWOUAAAAJ:roLk4NBRz8UC",
            1070,
            "https:\/\/static.usenix.org\/event\/fast08\/tech\/full_papers\/zhu\/zhu.pdf",
            "2324070564303386969",
            "\/scholar?cites=2324070564303386969",
            {
                "2007":4,
                "2008":12,
                "2009":30,
                "2010":61,
                "2011":75,
                "2012":107,
                "2013":110,
                "2014":101,
                "2015":116,
                "2016":122,
                "2017":84,
                "2018":88,
                "2019":68,
                "2020":63,
                "2021":2
            }
        ],
        [
            "Checkpointing is a simple technique for rollback recovery: the state of an executing program is periodically saved to a disk le from which it can be recovered after a failure. While recent research has developed a collection of powerful techniques for minimizing the overhead of writing checkpoint les, checkpointing remains unavailable to most application developers. In this paper we describe libckpt, a portable checkpointing tool for Unix that implements all applicable performance optimizations which are reported in the literature. While libckpt can be used in a mode which is almost totally transparent to the programmer, it also supports the incorporation of user directives into the creation of checkpoints. This user-directed checkpointing is an innovation which is unique to our work.",
            "Libckpt: Transparent checkpointing under unix",
            "James S Plank and Micah Beck and Gerry Kingsley and Kai Li",
            "1994",
            "9MSpWOUAAAAJ:9yKSN-GCB0IC",
            969,
            "https:\/\/www.usenix.org\/legacy\/publications\/library\/proceedings\/neworl\/full_papers\/plank.ps",
            "3214198437863435243",
            "\/scholar?cites=3214198437863435243",
            {
                "1995":10,
                "1996":17,
                "1997":25,
                "1998":31,
                "1999":34,
                "2000":34,
                "2001":31,
                "2002":38,
                "2003":47,
                "2004":65,
                "2005":57,
                "2006":39,
                "2007":58,
                "2008":54,
                "2009":55,
                "2010":61,
                "2011":54,
                "2012":41,
                "2013":38,
                "2014":32,
                "2015":29,
                "2016":28,
                "2017":17,
                "2018":19,
                "2019":14,
                "2020":14,
                "2021":2
            }
        ],
        [
            "Similarity indices for high-dimensional data are very desirable for building content-based search systems for feature-rich data such as audio, images, videos, and other sensor data. Recently, locality sensitive hashing (LSH) and its variations have been proposed as indexing techniques for approximate similarity search. A significant drawback of these approaches is the requirement for a large number of hash tables in order to achieve good search quality. This paper proposes a new indexing scheme called multi-probe LSH that overcomes this drawback. Multi-probe LSH is built on the well-known LSH technique, but it intelligently probes multiple buckets that are likely to contain query results in a hash table. Our method is inspired by and improves upon recent theoretical work on entropy-based LSH designed to reduce the space requirement of the basic LSH method. We have implemented the multi-probe LSH method and evaluated the implementation with two different high-dimensional datasets. Our evaluation shows that the multi-probe LSH method substantially improves upon previously proposed methods in both space and time efficiency. To achieve the same search quality, multi-probe LSH has a similar time-efficiency as the basic LSH method while reducing the number of hash tables by an order of magnitude. In comparison with the entropy-based LSH method, to achieve the same search quality, multi-probe LSH uses less query time and 5 to 8 times fewer number of hash tables.",
            "Multi-probe LSH: efficient indexing for high-dimensional similarity search",
            "Qin Lv and William Josephson and Zhe Wang and Moses Charikar and Kai Li",
            "2007",
            "9MSpWOUAAAAJ:ULOm3_A8WrAC",
            905,
            "https:\/\/collaborate.princeton.edu\/en\/publications\/multi-probe-lsh-efficient-indexing-for-high-dimensional-similarit",
            "2421787529638583289",
            "\/scholar?cites=2421787529638583289",
            {
                "2007":3,
                "2008":12,
                "2009":26,
                "2010":36,
                "2011":46,
                "2012":59,
                "2013":88,
                "2014":87,
                "2015":89,
                "2016":91,
                "2017":101,
                "2018":107,
                "2019":78,
                "2020":59,
                "2021":4
            }
        ],
        [
            "The network interfaces of existing multicomputers require a significant amount of software overhead to provide protection and to implement message passing protocols. This paper describes the design of a low-latency, high-bandwidth, virtual memory-mapped network interface for the SHRIMP multicomputer project at Princeton University. Without sacrificing protection, the network interface achieves low latency by using virtual memory mapping and write-latency hiding techniques, and obtains high bandwidth by providing a user-level block data transfer mechanism. We have implemented several message passing primitives in an experimental environment, demonstrating that our approach can reduce the message passing overhead to a few user-level instructions.",
            "Virtual memory mapped network interface for the SHRIMP multicomputer",
            "Matthias A Blumrich and Kai Li and Richard Alpert and Cezary Dubnicki and Edward W Felten and Jonathan Sandberg",
            "1994",
            "9MSpWOUAAAAJ:qjMakFHDy7sC",
            599,
            "https:\/\/dl.acm.org\/doi\/abs\/10.1145\/192007.192024",
            "6540897938681585189",
            "\/scholar?cites=6540897938681585189",
            {
                "1994":38,
                "1995":39,
                "1996":51,
                "1997":79,
                "1998":68,
                "1999":57,
                "2000":29,
                "2001":36,
                "2002":29,
                "2003":27,
                "2004":12,
                "2005":11,
                "2006":12,
                "2007":22,
                "2008":4,
                "2009":8,
                "2010":6,
                "2011":9,
                "2012":8,
                "2013":5,
                "2014":6,
                "2015":6,
                "2016":6,
                "2017":5,
                "2018":2,
                "2019":2,
                "2020":1
            }
        ],
        [
            "Image classification is a critical task for both humans and computers. One of the challenges lies in the large scale of the semantic space. In particular, humans can recognize tens of thousands of object classes and scenes. No computer vision algorithm today has been tested at this scale. This paper presents a study of large scale categorization including a series of challenging experiments on classification with more than 10,000 image classes. We find that a) computational issues become crucial in algorithm design; b) conventional wisdom from a couple of hundred image categories on relative performance of different classifiers does not necessarily hold when the number of categories increases; c) there is a surprisingly strong relationship between the structure of WordNet (developed for studying language) and the difficulty of visual categorization; d) classification can be improved by exploiting the \u2026",
            "What does classifying more than 10,000 image categories tell us?",
            "Jia Deng and Alexander C Berg and Kai Li and Li Fei-Fei",
            "2010",
            "9MSpWOUAAAAJ:NMxIlDl6LWMC",
            589,
            "https:\/\/link.springer.com\/chapter\/10.1007\/978-3-642-15555-0_6",
            "11087452830052725800",
            "\/scholar?cites=11087452830052725800",
            {
                "2009":5,
                "2010":3,
                "2011":34,
                "2012":58,
                "2013":83,
                "2014":84,
                "2015":62,
                "2016":60,
                "2017":49,
                "2018":54,
                "2019":33,
                "2020":48,
                "2021":2
            }
        ]
    ]
}
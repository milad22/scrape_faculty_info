{
    "columns":[
        "abstract",
        "title",
        "author",
        "pub_year",
        "author_pub_id",
        "num_citations",
        "pub_url",
        "cites_id",
        "citedby_url",
        "cites_per_year"
    ],
    "index":[
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
    ],
    "data":[
        [
            "We consider clustering problems in which the available attributes can be split into two independent subsets, such that either subset suffices for learning. Example applications of this multi-view setting include clustering of web pages which have an intrinsic view (the pages themselves) and an extrinsic view (eg, anchor texts of inbound hyperlinks); multi-view learning has so far been studied in the context of classification. We develop and study partitioning and agglomerative, hierarchical multi-view clustering algorithms for text data. We find empirically that the multiview versions of k-Means and EM greatly improve on their single-view counterparts. By contrast, we obtain negative results for agglomerative hierarchical multi-view clustering. Our analysis explains this surprising phenomenon.",
            "Multi-view clustering.",
            "Steffen Bickel and Tobias Scheffer",
            "2004",
            "UjV0M9QAAAAJ:8k81kl-MbHgC",
            708,
            "https:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.66.312&rep=rep1&type=pdf",
            "2171897181865317994",
            "\/scholar?cites=2171897181865317994",
            {
                "2005":14,
                "2006":12,
                "2007":8,
                "2008":15,
                "2009":23,
                "2010":27,
                "2011":23,
                "2012":42,
                "2013":43,
                "2014":52,
                "2015":56,
                "2016":71,
                "2017":81,
                "2018":76,
                "2019":74,
                "2020":75,
                "2021":6
            }
        ],
        [
            "Information extraction from HTML documents requires a classifier capable of assigning semantic labels to the words or word sequences to be extracted. If completely labeled documents are available for training, well-known Markov model techniques can be used to learn such classifiers. In this paper, we consider the more challenging task of learning hidden Markov models (HMMs) when only partially (sparsely) labeled documents are available for training. We first give detailed account of the task and its appropriate loss function, and show how it can be minimized given an HMM. We describe an EM style algorithm for learning HMMs from partially labeled data. We then present an active learning algorithm that selects \u201cdifficult\u201d unlabeled tokens and asks the user to label them. We study empirically by how much active learning reduces the required data labeling effort, or increases the quality of the learned \u2026",
            "Active hidden markov models for information extraction",
            "Tobias Scheffer and Christian Decomain and Stefan Wrobel",
            "2001",
            "UjV0M9QAAAAJ:70eg2SAEIzsC",
            406,
            "https:\/\/link.springer.com\/chapter\/10.1007\/3-540-44816-0_31",
            "15702698121721563954",
            "\/scholar?cites=15702698121721563954",
            {
                "2004":7,
                "2005":11,
                "2006":8,
                "2007":7,
                "2008":14,
                "2009":20,
                "2010":13,
                "2011":17,
                "2012":17,
                "2013":23,
                "2014":18,
                "2015":29,
                "2016":24,
                "2017":39,
                "2018":39,
                "2019":49,
                "2020":67,
                "2021":1
            }
        ],
        [
            "We address classification problems for which the training instances are governed by a distribution that is allowed to differ arbitrarily from the test distribution---problems also referred to as classification under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. We formulate the general problem of learning under covariate shift as an integrated optimization problem. We derive a kernel logistic regression classifier for differing training and test distributions.",
            "Discriminative learning for differing training and test distributions",
            "Steffen Bickel and Michael Br\u00fcckner and Tobias Scheffer",
            "2007",
            "UjV0M9QAAAAJ:Zph67rFs4hoC",
            390,
            "https:\/\/dl.acm.org\/doi\/abs\/10.1145\/1273496.1273507",
            "3400144996130326939",
            "\/scholar?cites=3400144996130326939",
            {
                "2006":1,
                "2007":5,
                "2008":26,
                "2009":35,
                "2010":28,
                "2011":22,
                "2012":34,
                "2013":23,
                "2014":23,
                "2015":27,
                "2016":26,
                "2017":11,
                "2018":24,
                "2019":50,
                "2020":40,
                "2021":5
            }
        ],
        [
            "Publication repositories contain an abundance of information about the evolution of scientific research areas. We address the problem of creating a visualization of a research area that describes the flow of topics between papers, quantifies the impact that papers have on each other, and helps to identify key contributions. To this end, we devise a probabilistic topic model that explains the generation of documents; the model incorporates the aspects of topical innovation and topical inheritance via citations. We evaluate the model's ability to predict the strength of influence of citations against manually rated citations.",
            "Unsupervised prediction of citation influences",
            "Laura Dietz and Steffen Bickel and Tobias Scheffer",
            "2007",
            "UjV0M9QAAAAJ:ZeXyd9-uunAC",
            316,
            "https:\/\/dl.acm.org\/doi\/abs\/10.1145\/1273496.1273526",
            "5875628951814698019",
            "\/scholar?cites=5875628951814698019",
            {
                "2007":2,
                "2008":9,
                "2009":18,
                "2010":18,
                "2011":23,
                "2012":26,
                "2013":25,
                "2014":43,
                "2015":34,
                "2016":37,
                "2017":21,
                "2018":21,
                "2019":13,
                "2020":21,
                "2021":2
            }
        ],
        [
            "We address classification problems for which the training instances are governed by an input distribution that is allowed to differ arbitrarily from the test distribution\u2014problems also referred to as classification under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. The problem of learning under covariate shift can be written as an integrated optimization problem. Instantiating the general optimization problem leads to a kernel logistic regression and an exponential model classifier for covariate shift. The optimization problem is convex under certain conditions; our findings also clarify the relationship to the known kernel mean matching procedure. We report on experiments on problems of spam filtering, text classification, and landmine detection.",
            "Discriminative learning under covariate shift.",
            "Steffen Bickel and Michael Br\u00fcckner and Tobias Scheffer",
            "2009",
            "UjV0M9QAAAAJ:dfsIfKJdRG4C",
            277,
            "https:\/\/www.jmlr.org\/papers\/volume10\/bickel09a\/bickel09a.pdf",
            "18207159089237933167",
            "\/scholar?cites=18207159089237933167",
            {
                "2008":1,
                "2009":3,
                "2010":5,
                "2011":19,
                "2012":26,
                "2013":20,
                "2014":11,
                "2015":18,
                "2016":24,
                "2017":26,
                "2018":36,
                "2019":30,
                "2020":46,
                "2021":4
            }
        ],
        [
            "In many applications, unlabelled examples are inexpensive and easy to obtain. Semi-supervised approaches try to utilise such examples to reduce the predictive error. In this paper, we investigate a semi-supervised least squares regression algorithm based on the co-learning approach. Similar to other semi-supervised algorithms, our base algorithm has cubic runtime complexity in the number of unlabelled examples. To be able to handle larger sets of unlabelled examples, we devise a semi-parametric variant that scales linearly in the number of unlabelled examples. Experiments show a significant error reduction by co-regularisation and a large runtime improvement for the semi-parametric approximation. Last but not least, we propose a distributed procedure that can be applied without collecting all data at a single site.",
            "Efficient co-regularised least squares regression",
            "Ulf Brefeld and Thomas G\u00e4rtner and Tobias Scheffer and Stefan Wrobel",
            "2006",
            "UjV0M9QAAAAJ:4OULZ7Gr8RgC",
            207,
            "https:\/\/dl.acm.org\/doi\/abs\/10.1145\/1143844.1143862",
            "17939650556671663886",
            "\/scholar?cites=17939650556671663886",
            {
                "2005":1,
                "2006":2,
                "2007":10,
                "2008":12,
                "2009":15,
                "2010":14,
                "2011":13,
                "2012":23,
                "2013":21,
                "2014":12,
                "2015":15,
                "2016":16,
                "2017":21,
                "2018":11,
                "2019":10,
                "2020":8
            }
        ],
        [
            "Multi-view algorithms, such as co-training and co-EM, utilize unlabeled data when the available attributes can be split into independent and compatible subsets. Co-EM outperforms co-training for many problems, but it requires the underlying learner to estimate class probabilities, and to learn from probabilistically labeled data. Therefore, co-EM has so far only been studied with naive Bayesian learners. We cast linear classifiers into a probabilistic framework and develop a co-EM version of the Support Vector Machine. We conduct experiments on text classification problems and compare the family of semi-supervised support vector algorithms under different conditions, including violations of the assumptions underlying multi-view learning. For some problems, such as course web page classification, we observe the most accurate results reported so far.",
            "Co-EM support vector learning",
            "Ulf Brefeld and Tobias Scheffer",
            "2004",
            "UjV0M9QAAAAJ:bEWYMUwI8FkC",
            207,
            "https:\/\/dl.acm.org\/doi\/abs\/10.1145\/1015330.1015350",
            "10262873274445096144",
            "\/scholar?cites=10262873274445096144",
            {
                "2004":2,
                "2005":16,
                "2006":11,
                "2007":3,
                "2008":12,
                "2009":9,
                "2010":11,
                "2011":11,
                "2012":20,
                "2013":21,
                "2014":12,
                "2015":18,
                "2016":10,
                "2017":16,
                "2018":8,
                "2019":9,
                "2020":13,
                "2021":2
            }
        ],
        [
            "When evaluating association rules, rules that differ in both support and confidence have to compared; a larger support has to be traded against a higher confidence. The solution which we propose for this problem is to maximize the expected accuracy that the association rule will have for future data. In a Bayesian framework, we determine the contributions of confidence and support to the expected accuracy on future data. We present a fast algorithm that finds the n best rules which maximize the resulting criterion. The algorithm dynamically prunes redundant rules and parts of the hypothesis space that cannot contain better solutions than the best ones found so far. We evaluate the performance of the algorithm (relative to the Apriori algorithm) on realistic knowledge discovery problems.",
            "Finding association rules that trade support optimally against confidence",
            "Tobias Scheffer",
            "2001",
            "UjV0M9QAAAAJ:hMod-77fHWUC",
            198,
            "https:\/\/link.springer.com\/chapter\/10.1007\/3-540-44794-6_35",
            "12242645118356378165",
            "\/scholar?cites=12242645118356378165",
            {
                "2002":3,
                "2003":1,
                "2004":4,
                "2005":7,
                "2006":7,
                "2007":7,
                "2008":9,
                "2009":15,
                "2010":10,
                "2011":12,
                "2012":11,
                "2013":12,
                "2014":7,
                "2015":18,
                "2016":21,
                "2017":12,
                "2018":17,
                "2019":9,
                "2020":10,
                "2021":1
            }
        ],
        [
            "We address the problem of learning classifiers for a large number of tasks. We derive a solution that produces resampling weights which match the pool of all examples to the target distribution of any given task. Our work is motivated by the problem of predicting the outcome of a therapy attempt for a patient who carries an HIV virus with a set of observed genetic properties. Such predictions need to be made for hundreds of possible combinations of drugs, some of which use similar biochemical mechanisms. Multi-task learning enables us to make predictions even for drug combinations with few or no training examples and substantially improves the overall prediction accuracy.",
            "Multi-task learning for HIV therapy screening",
            "Steffen Bickel and Jasmina Bogojeska and Thomas Lengauer and Tobias Scheffer",
            "2008",
            "UjV0M9QAAAAJ:NMxIlDl6LWMC",
            183,
            "https:\/\/dl.acm.org\/doi\/abs\/10.1145\/1390156.1390164",
            "12660146601276985371",
            "\/scholar?cites=12660146601276985371",
            {
                "2008":2,
                "2009":12,
                "2010":16,
                "2011":21,
                "2012":20,
                "2013":11,
                "2014":7,
                "2015":10,
                "2016":16,
                "2017":21,
                "2018":16,
                "2019":14,
                "2020":13,
                "2021":2
            }
        ],
        [
            "The standard assumption of identically distributed training and test data is violated when test data are generated in response to a predictive model. This becomes apparent, for example, in the context of email spam filtering, where an email service provider employs a spam filter and the spam sender can take this filter into account when generating new emails. We model the interaction between learner and data generator as a Stackelberg competition in which the learner plays the role of the leader and the data generator may react on the leader's move. We derive an optimization problem to determine the solution of this game and present several instances of the Stackelberg prediction game. We show that the Stackelberg prediction game generalizes existing prediction models. Finally, we explore properties of the discussed models empirically in the context of email spam filtering.",
            "Stackelberg games for adversarial prediction problems",
            "Michael Br\u00fcckner and Tobias Scheffer",
            "2011",
            "UjV0M9QAAAAJ:yD5IFk8b50cC",
            179,
            "https:\/\/dl.acm.org\/doi\/abs\/10.1145\/2020408.2020495",
            "18256748471095531457",
            "\/scholar?cites=18256748471095531457",
            {
                "2012":8,
                "2013":7,
                "2014":8,
                "2015":8,
                "2016":27,
                "2017":24,
                "2018":30,
                "2019":24,
                "2020":37,
                "2021":4
            }
        ]
    ]
}
{
    "columns":[
        "abstract",
        "title",
        "author",
        "pub_year",
        "author_pub_id",
        "num_citations",
        "pub_url",
        "cites_id",
        "citedby_url",
        "cites_per_year"
    ],
    "index":[
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
    ],
    "data":[
        [
            "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.",
            "Image-to-image translation with conditional adversarial networks",
            "Phillip Isola and Jun-Yan Zhu and Tinghui Zhou and Alexei A Efros",
            "2017",
            "d97bGd8AAAAJ:0klj8wIChNAC",
            7991,
            "http:\/\/openaccess.thecvf.com\/content_cvpr_2017\/html\/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.html",
            "16757839449706651543",
            "\/scholar?cites=16757839449706651543",
            {
                "2017":370,
                "2018":1420,
                "2019":2547,
                "2020":3291,
                "2021":253
            }
        ],
        [
            "Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G: X-> Y such that the distribution of images from G (X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F: Y-> X and introduce a cycle consistency loss to push F (G (X))~ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.",
            "Unpaired image-to-image translation using cycle-consistent adversarial networks",
            "Jun-Yan Zhu and Taesung Park and Phillip Isola and Alexei A Efros",
            "2017",
            "d97bGd8AAAAJ:Q17yWvk9gpwC",
            7347,
            "http:\/\/openaccess.thecvf.com\/content_iccv_2017\/html\/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html",
            "18396328236259959400",
            "\/scholar?cites=18396328236259959400",
            {
                "2017":188,
                "2018":1145,
                "2019":2387,
                "2020":3290,
                "2021":248
            }
        ],
        [
            "A non-parametric method for texture synthesis is proposed. The texture synthesis process grows a new image outward from an initial seed, one pixel at a time. A Markov random field model is assumed, and the conditional distribution of a pixel given all its neighbors synthesized so far is estimated by querying the sample image and finding all similar neighborhoods. The degree of randomness is controlled by a single perceptually intuitive parameter. The method aims at preserving as much local structure as possible and produces good results for a wide variety of synthetic and real-world textures.",
            "Texture synthesis by non-parametric sampling",
            "Alexei A Efros and Thomas K Leung",
            "1999",
            "d97bGd8AAAAJ:u5HHmVD_uO8C",
            4040,
            "https:\/\/ieeexplore.ieee.org\/abstract\/document\/790383\/",
            "2677654440732607660",
            "\/scholar?cites=2677654440732607660",
            {
                "2000":22,
                "2001":46,
                "2002":71,
                "2003":104,
                "2004":146,
                "2005":189,
                "2006":205,
                "2007":238,
                "2008":202,
                "2009":252,
                "2010":238,
                "2011":246,
                "2012":219,
                "2013":224,
                "2014":261,
                "2015":206,
                "2016":215,
                "2017":229,
                "2018":219,
                "2019":237,
                "2020":195,
                "2021":17
            }
        ],
        [
            "We present a simple image-based method of generating novel visual appearance in which a new image is synthesized by stitching together small patches of existing images. We call this process image quilting. First, we use quilting as a fast and very simple texture synthesis algorithm which produces surprisingly good results for a wide range of textures. Second, we extend the algorithm to perform texture transfer\u2014rendering an object with a texture taken from a different object. More generally, we demonstrate how an image can be re-rendered in the style of a different image. The method works directly on the images and does not require 3D information.",
            "Image quilting for texture synthesis and transfer",
            "Alexei A Efros and William T Freeman",
            "2001",
            "d97bGd8AAAAJ:u-x6o8ySG0sC",
            2981,
            "https:\/\/dl.acm.org\/doi\/abs\/10.1145\/383259.383296",
            "1322760425232869240",
            "\/scholar?cites=1322760425232869240",
            {
                "2001":13,
                "2002":48,
                "2003":80,
                "2004":120,
                "2005":145,
                "2006":177,
                "2007":174,
                "2008":161,
                "2009":174,
                "2010":185,
                "2011":140,
                "2012":170,
                "2013":145,
                "2014":157,
                "2015":151,
                "2016":144,
                "2017":189,
                "2018":175,
                "2019":187,
                "2020":185,
                "2021":17
            }
        ],
        [
            "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders--a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part (s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.",
            "Context Encoders: Feature Learning by Inpainting",
            "Deepak Pathak and Philipp Krahenbuhl and Jeff Donahue and Trevor Darrell and Alexei A Efros",
            "2016",
            "d97bGd8AAAAJ:gxb_f1p9zx4C",
            2492,
            "http:\/\/openaccess.thecvf.com\/content_cvpr_2016\/html\/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html",
            "11404163095581754770",
            "\/scholar?cites=11404163095581754770",
            {
                "2016":28,
                "2017":209,
                "2018":529,
                "2019":736,
                "2020":865,
                "2021":74
            }
        ],
        [
            "We seek to discover the object categories depicted in a set of unlabelled images. We achieve this using a model developed in the statistical text literature: probabilistic latent semantic analysis (pLSA). In text analysis, this is used to discover topics in a corpus using the bag-of-words document representation. Here we treat object categories as topics, so that an image containing instances of several categories is modeled as a mixture of topics. The model is applied to images by using a visual analogue of a word, formed by vector quantizing SIFT-like region descriptors. The topic discovery approach successfully translates to the visual domain: for a small set of objects, we show that both the object categories and their approximate spatial layout are found without supervision. Performance of this unsupervised method is compared to the supervised approach of Fergus et al. (2003) on a set of unseen images containing \u2026",
            "Discovering objects and their location in images",
            "Josef Sivic and Bryan C Russell and Alexei A Efros and Andrew Zisserman and William T Freeman",
            "2005",
            "d97bGd8AAAAJ:9yKSN-GCB0IC",
            1840,
            "https:\/\/ieeexplore.ieee.org\/abstract\/document\/1541280\/",
            "860265209767077576",
            "\/scholar?cites=860265209767077576",
            {
                "2004":5,
                "2005":15,
                "2006":73,
                "2007":121,
                "2008":148,
                "2009":189,
                "2010":201,
                "2011":176,
                "2012":163,
                "2013":154,
                "2014":131,
                "2015":115,
                "2016":92,
                "2017":74,
                "2018":46,
                "2019":55,
                "2020":40,
                "2021":1
            }
        ],
        [
            "Our goal is to recognize human action at a distance, at resolutions where a whole person may be, say, 30 pixels tall. We introduce a novel motion descriptor based on optical flow measurements in a spatiotemporal volume for each stabilized human figure, and an associated similarity measure to be used in a nearest-neighbor framework. Making use of noisy optical flow measurements is the key challenge, which is addressed by treating optical flow not as precise pixel displacements, but rather as a spatial pattern of noisy measurements which are carefully smoothed and aggregated to form our spatiotemporal motion descriptor. To classify the action being performed by a human figure in a query sequence, we retrieve nearest neighbor(s) from a database of stored, annotated video sequences. We can also use these retrieved exemplars to transfer 2D\/3D skeletons onto the figures in the query sequence, as well as \u2026",
            "Recognizing action at a distance",
            "Alexei A Efros and Alexander C Berg and Greg Mori and Jitendra Malik",
            "2003",
            "d97bGd8AAAAJ:d1gkVwhDpl0C",
            1725,
            "https:\/\/ieeexplore.ieee.org\/abstract\/document\/1238420\/",
            "5814232222109242460",
            "\/scholar?cites=5814232222109242460",
            {
                "2004":42,
                "2005":55,
                "2006":59,
                "2007":87,
                "2008":147,
                "2009":156,
                "2010":163,
                "2011":141,
                "2012":137,
                "2013":158,
                "2014":158,
                "2015":82,
                "2016":103,
                "2017":74,
                "2018":50,
                "2019":51,
                "2020":26,
                "2021":3
            }
        ],
        [
            "Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set \u2026",
            "Unbiased look at dataset bias",
            "Antonio Torralba and Alexei A Efros",
            "2011",
            "d97bGd8AAAAJ:L8Ckcad2t8MC",
            1539,
            "https:\/\/ieeexplore.ieee.org\/abstract\/document\/5995347\/",
            "6282021422333855390",
            "\/scholar?cites=6282021422333855390",
            {
                "2010":4,
                "2011":7,
                "2012":46,
                "2013":75,
                "2014":121,
                "2015":126,
                "2016":154,
                "2017":157,
                "2018":186,
                "2019":262,
                "2020":330,
                "2021":27
            }
        ],
        [
            "Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a \u201ccolorization Turing test,\u201d asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32 % of the trials, significantly higher than \u2026",
            "Colorful Image Colorization",
            "Richard Zhang and Phillip Isola and Alexei A Efros",
            "2016",
            "d97bGd8AAAAJ:nnITTVbzT6kC",
            1533,
            "https:\/\/link.springer.com\/chapter\/10.1007\/978-3-319-46487-9_40",
            "7106912543889350821",
            "\/scholar?cites=7106912543889350821",
            {
                "2016":27,
                "2017":164,
                "2018":280,
                "2019":384,
                "2020":597,
                "2021":48
            }
        ],
        [
            "Image understanding requires not only individually estimating elements of the visual world but also capturing the interplay among them. In this paper, we provide a framework for placing local object detection in the context of the overall 3D scene by modeling the interdependence of objects, surface orientations, and camera viewpoint. Most object detection methods consider all scales and locations in the image as equally likely. We show that with probabilistic estimates of 3D geometry, both in terms of surfaces and world coordinates, we can put objects into perspective and model the scale and location variance in the image. Our approach reflects the cyclical nature of the problem by allowing probabilistic object hypotheses to refine geometry and vice-versa. Our framework allows painless substitution of almost any object detector and is easily extended to include other aspects of image understanding. Our \u2026",
            "Putting objects in perspective",
            "Derek Hoiem and Alexei A Efros and Martial Hebert",
            "2008",
            "d97bGd8AAAAJ:UeHWp8X0CEIC",
            1147,
            "https:\/\/link.springer.com\/content\/pdf\/10.1007\/s11263-008-0137-5.pdf",
            "9518897260515287060",
            "\/scholar?cites=9518897260515287060",
            {
                "2007":47,
                "2008":67,
                "2009":116,
                "2010":108,
                "2011":112,
                "2012":101,
                "2013":95,
                "2014":116,
                "2015":86,
                "2016":68,
                "2017":62,
                "2018":58,
                "2019":46,
                "2020":29,
                "2021":1
            }
        ]
    ]
}
{
    "columns":[
        "abstract",
        "title",
        "author",
        "pub_year",
        "author_pub_id",
        "num_citations",
        "pub_url",
        "cites_id",
        "citedby_url",
        "cites_per_year"
    ],
    "index":[
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
    ],
    "data":[
        [
            "An important component of routine visual behavior is the ability to find one item in a visual world filled with other, distracting items. This ability to performvisual search has been the subject of a large body of research in the past 15 years. This paper reviews the visual search literature and presents a model of human search behavior. Built upon the work of Neisser, Treisman, Julesz, and others, the model distinguishes between a preattentive, massively parallel stage that processes information about basic visual features (color, motion, various depth cues, etc.) across large portions of the visual field and a subsequent limited-capacity stage that performs other, more complex operations (e.g., face recognition, reading, object identification) over a limited portion of the visual field. The spatial deployment of the limited-capacity process is under attentional control. The heart of the guided search model is the idea that \u2026",
            "Guided search 2.0 a revised model of visual search",
            "Jeremy M Wolfe",
            "1994",
            "av3q_DgAAAAJ:u5HHmVD_uO8C",
            4095,
            "https:\/\/link.springer.com\/article\/10.3758\/BF03200774",
            "2326166330808419924",
            "\/scholar?cites=2326166330808419924",
            {
                "1995":21,
                "1996":43,
                "1997":47,
                "1998":80,
                "1999":93,
                "2000":87,
                "2001":104,
                "2002":101,
                "2003":118,
                "2004":139,
                "2005":146,
                "2006":163,
                "2007":153,
                "2008":161,
                "2009":189,
                "2010":193,
                "2011":218,
                "2012":201,
                "2013":242,
                "2014":248,
                "2015":222,
                "2016":227,
                "2017":213,
                "2018":203,
                "2019":206,
                "2020":212,
                "2021":14
            }
        ],
        [
            "Subjects searched sets of items for targets defined by conjunctions of color and form, color and orientation, or color and size. Set size was varied and reaction times (RTs) were measured. For many unpracticed subjects, the slopes of the resulting RT\u00d7 Set Size functions are too shallow to be consistent with A. Treisman's (see record 1988-00255-001) and A. Treisman and G. Gelade's (see record 1980-04680-001) feature integration model, which proposes serial, self-terminating search for conjunctions. Searches for triple conjunctions (Color\u00d7 Size\u00d7 Form) are easier than searches for standard conjunctions and can be independent of set size. A guided search model similar to JE Hoffman's (see record 1981-04707-001) two-stage model can account for these data. In the model, parallel processes use information about simple features to guide attention in the search for conjunctions. Triple conjunctions are found more \u2026",
            "Guided search: an alternative to the feature integration model for visual search.",
            "Jeremy M Wolfe and Kyle R Cave and Susan L Franzel",
            "1989",
            "av3q_DgAAAAJ:u-x6o8ySG0sC",
            2450,
            "https:\/\/psycnet.apa.org\/record\/1990-00306-001",
            "501761626849278654",
            "\/scholar?cites=501761626849278654",
            {
                "1989":29,
                "1990":32,
                "1991":23,
                "1992":67,
                "1993":35,
                "1994":55,
                "1995":42,
                "1996":51,
                "1997":45,
                "1998":59,
                "1999":70,
                "2000":60,
                "2001":75,
                "2002":68,
                "2003":65,
                "2004":76,
                "2005":73,
                "2006":77,
                "2007":92,
                "2008":78,
                "2009":103,
                "2010":105,
                "2011":94,
                "2012":109,
                "2013":107,
                "2014":120,
                "2015":118,
                "2016":83,
                "2017":131,
                "2018":81,
                "2019":102,
                "2020":98,
                "2021":4
            }
        ],
        [
            "As you drive into the centre of town, cars and trucks approach from several directions, and pedestrians swarm into the intersection. The wind blows a newspaper into the gutter and a pigeon does something unexpected on your windshield. This would be a demanding and stressful situation, but you would probably make it to the other side of town without mishap. Why is this situation taxing, and how do you cope?",
            "What attributes guide the deployment of visual attention and how do they do it?",
            "Jeremy M Wolfe and Todd S Horowitz",
            "2004",
            "av3q_DgAAAAJ:2osOgNQ5qMEC",
            1851,
            "https:\/\/www.nature.com\/articles\/nrn1411",
            "4210742534495126791",
            "\/scholar?cites=4210742534495126791",
            {
                "2005":19,
                "2006":34,
                "2007":56,
                "2008":67,
                "2009":72,
                "2010":90,
                "2011":121,
                "2012":120,
                "2013":162,
                "2014":164,
                "2015":171,
                "2016":153,
                "2017":186,
                "2018":117,
                "2019":150,
                "2020":131,
                "2021":12
            }
        ],
        [
            "In generalizing from laboratory search tasks to tasks in the real world, we need to be clever and careful. In a typical laboratory search experiment, observers (Os) with very little at stake typically do hundreds of trials of one type of search with each search lasting on the order of a second. The search stimuli and the response are generally quite artificial, and the search target is typically present on half of the trials or on all of them if the response is some sort of localization or identification task. These tasks are intended to tell us about search in the world where, in contrast to the lab, the stakes could be very high.(Where is my child? Is there a bomb here?) Unlike in the lab, most real tasks will not be repeated over and over again. How often do you need to find the baking powder on the supermarket shelf? Unlike a single search trial in the lab, many real search tasks take quite a while. For instance, searching a patient's \u2026",
            "Visual search.",
            "Jeremy M Wolfe",
            "2015",
            "av3q_DgAAAAJ:d1gkVwhDpl0C",
            1654,
            "https:\/\/psycnet.apa.org\/record\/2016-08577-002",
            "11250285125561010",
            "\/scholar?cites=11250285125561010",
            {
                "1998":9,
                "1999":18,
                "2000":24,
                "2001":48,
                "2002":44,
                "2003":64,
                "2004":73,
                "2005":76,
                "2006":95,
                "2007":100,
                "2008":85,
                "2009":98,
                "2010":96,
                "2011":104,
                "2012":73,
                "2013":94,
                "2014":79,
                "2015":84,
                "2016":59,
                "2017":110,
                "2018":59,
                "2019":70,
                "2020":62,
                "2021":7
            }
        ],
        [
            "Visual input is processed in parallel in the early stages of the visual system. Later, object recognition processes are also massively parallel, matching a visual object with a vast array of stored representation. A tight bottleneck in processing lies between these stages. It permits only one or a few visual objects at any one time to be submitted for recognition. That bottleneck limits performance on visual search tasks when an observer looks for one object in a field containing distracting objects. Guided Search is a model of the workings of that bottleneck. It proposes that a limited set of attributes, derived from early vision, can be used to guide the selection of visual objects. The bottleneck and recognition processes are modeled using an asynchronous version of a diffusion process. The current version (Guided Search 4.0) captures a range of empirical findings.Guided Search (GS) is a model of human visual search performance, specifically of search tasks in which an observer looks for a target object among some number of distracting items. Classically, models have described two mechanisms of search: serial and parallel (Egeth, 1966). In serial search, attention is directed to one item at a time, allowing each item to be classified as a target or a distractor in turn (Sternberg, 1966). Parallel models propose that all (or many) items are processed at the same time. A decision about target presence is based on the output of this processing (Neisser, 1963). GS evolved out of the two-stage architecture of models like Treisman\u2019s feature integration theory (FIT; Treisman & Gelade, 1980). FIT proposed a parallel, preattentive first stage and a serial second stage \u2026",
            "Guided search 4.0",
            "Jeremy M Wolfe and W Gray",
            "2007",
            "av3q_DgAAAAJ:eQOLeE2rZwMC",
            1051,
            "http:\/\/books.google.com\/books?hl=en&lr=&id=eB9nDAAAQBAJ&oi=fnd&pg=PA99&dq=info:o_h6eg8iLZ4J:scholar.google.com&ots=zUOU3BcqKD&sig=o7BleYxY7G5X4cHJahxdV19J3qc",
            "11397803681819326627",
            "\/scholar?cites=11397803681819326627",
            {
                "1999":13,
                "2000":4,
                "2001":6,
                "2002":7,
                "2003":10,
                "2004":4,
                "2005":10,
                "2006":13,
                "2007":15,
                "2008":16,
                "2009":44,
                "2010":49,
                "2011":52,
                "2012":59,
                "2013":78,
                "2014":103,
                "2015":89,
                "2016":78,
                "2017":127,
                "2018":86,
                "2019":90,
                "2020":77,
                "2021":8
            }
        ],
        [
            "In a typical visual search experiment, observers look through a set of items for a designated target that may or may not be present. Reaction time (RT) is measured as a function of the number of items in the display (set size), and inferences about the underlying search processes are based on the slopes of the resulting RT x Set Size functions. Most search experiments involve 5 to 15 subjects performing a few hundred trials each. In this retrospective study, I examine results from 2,500 experimental sessions of a few hundred trials each (approximately 1 million total trials). These data represent a wide variety of search tasks. The resulting picture of human search behavior requires changes in our theories of visual search.",
            "What can 1 million trials tell us about visual search?",
            "Jeremy M Wolfe",
            "1998",
            "av3q_DgAAAAJ:qjMakFHDy7sC",
            833,
            "https:\/\/journals.sagepub.com\/doi\/abs\/10.1111\/1467-9280.00006",
            "10006834996884196897",
            "\/scholar?cites=10006834996884196897",
            {
                "1998":3,
                "1999":8,
                "2000":13,
                "2001":22,
                "2002":18,
                "2003":18,
                "2004":25,
                "2005":31,
                "2006":32,
                "2007":36,
                "2008":30,
                "2009":41,
                "2010":34,
                "2011":56,
                "2012":52,
                "2013":45,
                "2014":50,
                "2015":65,
                "2016":45,
                "2017":71,
                "2018":37,
                "2019":43,
                "2020":44,
                "2021":1
            }
        ],
        [
            "Treisman's Feature Integration Theory and Julesz's Texton Theory explain many aspects of visual search. However, these theories require that parallel processing mechanisms not be used in many visual searches for which they would be useful, and they imply that visual processing should be much slower than it is. Most importantly, they cannot account for recent data showing that some subjects can perform some conjunction searches very efficiently. Feature Integration Theory can be modified so that it accounts for these data and helps to answer these questions. In this new theory, which we call Guided Search, the parallel stage guides the serial stage as it chooses display elements to process. A computer simulation of Guided Search produces the same general patterns as human subjects in a number of different types of visual search.",
            "Modeling the role of parallel processing in visual search",
            "Kyle R Cave and Jeremy M Wolfe",
            "1990",
            "av3q_DgAAAAJ:9yKSN-GCB0IC",
            830,
            "https:\/\/www.sciencedirect.com\/science\/article\/pii\/001002859090017X",
            "5380840726702538140",
            "\/scholar?cites=5380840726702538140",
            {
                "1990":25,
                "1991":11,
                "1992":23,
                "1993":28,
                "1994":30,
                "1995":23,
                "1996":32,
                "1997":19,
                "1998":42,
                "1999":48,
                "2000":33,
                "2001":32,
                "2002":29,
                "2003":27,
                "2004":40,
                "2005":27,
                "2006":36,
                "2007":32,
                "2008":26,
                "2009":29,
                "2010":34,
                "2011":25,
                "2012":27,
                "2013":27,
                "2014":21,
                "2015":17,
                "2016":17,
                "2017":21,
                "2018":9,
                "2019":12,
                "2020":12,
                "2021":1
            }
        ],
        [
            "Humans spend a lot of time searching for things, such as roadside traffic signs 1, soccer balls 2 or tumours in mammograms 3. These tasks involve the deployment of attention from one item in the visual field to the next. Common sense suggests that rejected items should be noted in some fashion so that effort is not expended in re-examining items that have been attended to and rejected. However, common sense is wrong. Here we asked human observers to search for a letter \u2018T\u2019among letters \u2018L\u2019. This search demandsvisual attention and normally proceeds at a rate of 20\u201330 milliseconds per item 4. In the critical condition, we randomly relocated all letters every 111 milliseconds. This made it impossible for the subjects to keep track of the progress of the search. Nevertheless, the efficiency of the search was unchanged. Theories of visual search all assume that search relies on accumulating information about the \u2026",
            "Visual search has no memory",
            "Todd S Horowitz and Jeremy M Wolfe",
            "1998",
            "av3q_DgAAAAJ:UeHWp8X0CEIC",
            702,
            "https:\/\/www.nature.com\/articles\/29068",
            "5516901110960618123",
            "\/scholar?cites=5516901110960618123",
            {
                "1998":2,
                "1999":10,
                "2000":27,
                "2001":69,
                "2002":32,
                "2003":39,
                "2004":34,
                "2005":36,
                "2006":39,
                "2007":43,
                "2008":29,
                "2009":28,
                "2010":31,
                "2011":19,
                "2012":35,
                "2013":25,
                "2014":27,
                "2015":16,
                "2016":25,
                "2017":62,
                "2018":13,
                "2019":25,
                "2020":21,
                "2021":4
            }
        ],
        [
            "This paper deals with the order in which different levels of form are recognized in a visual image. An experiment is reported in which the size of a tachistoscopically viewed image was varied. The results suggest neither an invariant \u201ctop-down\u201d (gross shapes first followed by lower-order details) or \u201cbottom-up\u201d (the opposite) sequence. Rather, they seem to suggest a sort of \u201cmiddle-out\u201d sequence: forms at some intermediate level of structure having an optimal size or spatial-frequency spectrum are processed first, with subsequent processing of both higher and lower levels of form.",
            "The order of visual processing:\u201cTop-down,\u201d\u201cbottom-up,\u201d or \u201cmiddle-out\u201d",
            "Ronald A Kinchla and Jeremy M Wolfe",
            "1979",
            "av3q_DgAAAAJ:IjCSPb-OGe4C",
            576,
            "https:\/\/link.springer.com\/article\/10.3758\/BF03202991",
            "14911152554011944858",
            "\/scholar?cites=14911152554011944858",
            {
                "1982":14,
                "1983":10,
                "1984":6,
                "1985":12,
                "1986":5,
                "1987":2,
                "1988":11,
                "1989":12,
                "1990":12,
                "1991":22,
                "1992":13,
                "1993":17,
                "1994":16,
                "1995":11,
                "1996":9,
                "1997":19,
                "1998":10,
                "1999":14,
                "2000":13,
                "2001":19,
                "2002":16,
                "2003":15,
                "2004":7,
                "2005":8,
                "2006":11,
                "2007":15,
                "2008":10,
                "2009":18,
                "2010":17,
                "2011":13,
                "2012":17,
                "2013":17,
                "2014":15,
                "2015":28,
                "2016":26,
                "2017":17,
                "2018":14,
                "2019":16,
                "2020":20,
                "2021":1
            }
        ],
        [
            "Observers, searching for targets among distractor items, guide attention with a mix of top-down information\u2014based on observers' knowledge\u2014and bottom-up information\u2014stimulus-based and largely independent of that knowledge. There are 2 types of top-down guidance: explicit information (eg, verbal description) and implicit priming by preceding targets (top-down because it implies knowledge of previous searches). Experiments 1 and 2 separate bottom-up and top-down contributions to singleton search. Experiment 3 shows that priming effects are based more strongly on target than on distractor identity. Experiments 4 and 5 show that more difficult search for one type of target (color) can impair search for other types (size, orientation). Experiment 6 shows that priming guides attention and does not just modulate response.",
            "Changing your mind: on the contributions of top-down and bottom-up guidance in visual search for feature singletons.",
            "Jeremy M Wolfe and Serena J Butcher and Carol Lee and Megan Hyle",
            "2003",
            "av3q_DgAAAAJ:W7OEmFMy1HYC",
            574,
            "https:\/\/psycnet.apa.org\/doiLanding?doi=10.1037\/0096-1523.29.2.483",
            "9648003381028451151",
            "\/scholar?cites=9648003381028451151",
            {
                "2004":14,
                "2005":39,
                "2006":29,
                "2007":37,
                "2008":29,
                "2009":27,
                "2010":41,
                "2011":38,
                "2012":37,
                "2013":47,
                "2014":34,
                "2015":37,
                "2016":32,
                "2017":32,
                "2018":28,
                "2019":31,
                "2020":36,
                "2021":2
            }
        ]
    ]
}
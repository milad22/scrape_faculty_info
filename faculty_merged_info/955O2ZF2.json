{
    "columns":[
        "id",
        "google_scholar_id",
        "titles",
        "abstracts",
        "interests"
    ],
    "index":[
        0
    ],
    "data":[
        [
            "955O2ZF2",
            "Q7OY_NkAAAAJ",
            [
                "Predicting the stability of large structured food webs",
                "Transition to chaos in random networks with cell-type-specific connectivity",
                "Analysis of Neuronal Spike Trains, Deconstructed",
                "Eigenvalues of block structured asymmetric random matrices",
                "The I1307K adenomatous polyposis coli gene variant does not contribute in the assessment of the risk for colorectal cancer in Ashkenazi Jews",
                "Low-dimensional dynamics of structured random networks",
                "Cerebellar learning using perturbations",
                "Optimal dynamic coding by mixed-dimensionality neurons in the head-direction system of bats",
                "Reduced synchronization persistence in neural networks derived from Atm-deficient mice",
                "Spike Triggered Covariance in Strongly Correlated Gaussian Stimuli"
            ],
            [
                "The stability of ecological systems has been a long-standing focus of ecology. Recently, tools from random matrix theory have identified the main drivers of stability in ecological communities whose network structure is random. However, empirical food webs differ greatly from random graphs. For example, their degree distribution is broader, they contain few trophic cycles, and they are almost interval. Here we derive an approximation for the stability of food webs whose structure is generated by the cascade model, in which \u2018larger\u2019species consume \u2018smaller\u2019ones. We predict the stability of these food webs with great accuracy, and our approximation also works well for food webs whose structure is determined empirically or by the niche model. We find that intervality and broad degree distributions tend to stabilize food webs, and that average interaction strength has little influence on stability, compared with the effect \u2026",
                "In neural circuits, statistical connectivity rules strongly depend on cell-type identity. We study dynamics of neural networks with cell-type-specific connectivity by extending the dynamic mean-field method and find that these networks exhibit a phase transition between silent and chaotic activity. By analyzing the locus of this transition, we derive a new result in random matrix theory: the spectral radius of a random connectivity matrix with block-structured variances. We apply our results to show how a small group of hyperexcitable neurons within the network can significantly increase the network\u2019s computational capacity by bringing it into the chaotic regime.",
                "As information flows through the brain, neuronal firing progresses from encoding the world as sensed by the animal to driving the motor output of subsequent behavior. One of the more tractable goals of quantitative neuroscience is to develop predictive models that relate the sensory or motor streams with neuronal firing. Here we review and contrast analytical tools used to accomplish this task. We focus on classes of models in which the external variable is compared with one or more feature vectors to extract a low-dimensional representation, the history of spiking and other variables are potentially incorporated, and these factors are nonlinearly transformed to predict the occurrences of spikes. We illustrate these techniques in application to datasets of different degrees of complexity. In particular, we address the fitting of models in the presence of strong correlations in the external variable, as occurs in natural \u2026",
                "We study the spectrum of an asymmetric random matrix with block structured variances. The rows and columns of the random square matrix are divided into D partitions with arbitrary size (linear in N). The parameters of the model are the variances of elements in each block, summarized in g\u2208R+D\u00d7D. Using the Hermitization approach and by studying the matrix-valued Stieltjes transform we show that these matrices have a circularly symmetric spectrum, we give an explicit formula for their spectral radius and a set of implicit equations for the full density function. We discuss applications of this model to neural networks.",
                "Ashkenazi Jews with the I1307K adenomatous polyposis coli gene variant were suggested to confer a higher risk for colorectal cancer (CRC). We assessed the clinical importance of this polymorphism in Israeli Jews at average and elevated risk for CRC. Among 1370 consecutive subjects that were examined, 975 Ashkenazi Jews were stratified into those at average risk (no personal or family history of colorectal neoplasia) and those at high risk. DNA was obtained from peripheral leukocytes and amplified by PCR, with primers designed to detect the I1307K variant. Overall, I1307K polymorphism was found in 7.1% (9.1% among Ashkenazi and 1.7% among non-Ashkenazi Jews). The carrier rate was 8.3 and 9.3% in average and high-risk Ashkenazim, respectively (P = 0.65). The overall odds ratio for neoplasia in carriers was 1.43 (95% confidence interval, 0.89\u20132.30). Age, gender, and the histopathological features \u2026",
                "Using a generalized random recurrent neural network model, and by extending our recently developed mean-field approach [J. Aljadeff, M. Stern, and T. Sharpee, Phys. Rev. Lett. 114, 088101 (2015)], we study the relationship between the network connectivity structure and its low-dimensional dynamics. Each connection in the network is a random number with mean 0 and variance that depends on pre-and postsynaptic neurons through a sufficiently smooth function g of their identities. We find that these networks undergo a phase transition from a silent to a chaotic state at a critical point we derive as a function of g. Above the critical point, although unit activation levels are chaotic, their autocorrelation functions are restricted to a low-dimensional subspace. This provides a direct link between the network's structure and some of its functional characteristics. We discuss example applications of the general results to \u2026",
                "The cerebellum aids the learning of fast, coordinated movements. According to current consensus, erroneously active parallel fibre synapses are depressed by complex spikes signalling movement errors. However, this theory cannot solve the credit assignment problem of processing a global movement evaluation into multiple cell-specific error signals. We identify a possible implementation of an algorithm solving this problem, whereby spontaneous complex spikes perturb ongoing movements, create eligibility traces and signal error changes guiding plasticity. Error changes are extracted by adaptively cancelling the average error. This framework, stochastic gradient descent with estimated global errors (SGDEGE), predicts synaptic plasticity rules that apparently contradict the current consensus but were supported by plasticity experiments in slices from mice under conditions designed to be physiological, highlighting the sensitivity of plasticity studies to experimental conditions. We analyse the algorithm\u2019s convergence and capacity. Finally, we suggest SGDEGE may also operate in the basal ganglia.",
                "Ethologically relevant stimuli are often multidimensional. In many brain systems, neurons with \u201cpure\u201d tuning to one stimulus dimension are found along with \u201cconjunctive\u201d neurons that encode several dimensions, forming an apparently redundant representation. Here we show using theoretical analysis that a mixed-dimensionality code can efficiently represent a stimulus in different behavioral regimes: encoding by conjunctive cells is more robust when the stimulus changes quickly, whereas on long timescales pure cells represent the stimulus more efficiently with fewer neurons. We tested our predictions experimentally in the bat head-direction system and found that many head-direction cells switched their tuning dynamically from pure to conjunctive representation as a function of angular velocity\u2014confirming our theoretical prediction. More broadly, our results suggest that optimal dimensionality depends on \u2026",
                "Many neurodegenerative diseases are characterized by malfunction of the DNA damage response. Therefore, it is important to understand the connection between system level neural network behavior and DNA. Neural networks drawn from genetically engineered animals, interfaced with micro-electrode arrays (MEA) allowed us to uncover the connections between networks' system level activity properties and such genome instability. We discovered that Atm protein deficiency, which in humans leads to progressive motor impairment, leads to a reduced synchronization persistence compared to wild type synchronization, after chemically imposed DNA damage. Not only does this suggest a role for DNA stability in neural network activity but also establishes an experimental paradigm for empirically determining the role a gene plays on the behavior of a neural network.",
                "Many biological systems perform computations on inputs that have very large dimensionality. Determining the relevant input combinations for a particular computation is often key to understanding its function. A common way to find the relevant input dimensions is to examine the difference in variance between the input distribution and the distribution of inputs associated with certain outputs. In systems neuroscience, the corresponding method is known as spike-triggered covariance (STC). This method has been highly successful in characterizing relevant input dimensions for neurons in a variety of sensory systems. So far, most studies used the STC method with weakly correlated Gaussian inputs. However, it is also important to use this method with inputs that have long range correlations typical of the natural sensory environment. In such cases, the stimulus covariance matrix has one (or more) outstanding eigenvalues that cannot be easily equalized because of sampling variability. Such outstanding modes interfere with analyses of statistical significance of candidate input dimensions that modulate neuronal outputs. In many cases, these modes obscure the significant dimensions. We show that the sensitivity of the STC method in the regime of strongly correlated inputs can be improved by an order of magnitude or more. This can be done by evaluating the significance of dimensions in the subspace orthogonal to the outstanding mode(s). Analyzing the responses of retinal ganglion cells probed with  Gaussian noise, we find that taking into account outstanding modes is crucial for recovering relevant input dimensions for these neurons."
            ],
            [
                [
                    "Theoretical Neuroscience",
                    "Computational Neuroscience",
                    "Statistical Physics"
                ]
            ]
        ]
    ]
}
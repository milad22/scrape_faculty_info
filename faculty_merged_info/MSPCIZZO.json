{
    "columns":[
        "id",
        "google_scholar_id",
        "titles",
        "abstracts",
        "interests",
        "personal_page_content"
    ],
    "index":[
        0
    ],
    "data":[
        [
            "MSPCIZZO",
            "AKHplAUAAAAJ",
            [
                "SMOTE: synthetic minority over-sampling technique",
                "SMOTEBoost: Improving prediction of the minority class in boosting",
                "Review of MR image segmentation techniques using pattern recognition",
                "Radiomics: the process and the challenges",
                "MRI segmentation: methods and applications",
                "A comparison of neural network and fuzzy clustering techniques in segmenting magnetic resonance images of the brain",
                "Automatic tumor segmentation using knowledge-based techniques",
                "Clustering with a genetically optimized approach",
                "Validity-guided (re) clustering with applications to image segmentation",
                "A comparison of decision tree ensemble creation techniques"
            ],
            [
                "An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of``normal''examples with only a small percentage of``abnormal''or``interesting''examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4. 5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.",
                "Many real world data mining applications involve learning from imbalanced data sets. Learning from data sets that contain very few instances of the minority (or interesting) class usually produces biased classifiers that have a higher predictive accuracy over the majority class(es), but poorer predictive accuracy over the minority class. SMOTE (Synthetic Minority Over-sampling TEchnique) is specifically designed for learning from imbalanced data sets. This paper presents a novel approach for learning from imbalanced data sets, based on a combination of the SMOTE algorithm and the boosting procedure. Unlike standard boosting where all misclassified examples are given equal weights, SMOTEBoost creates synthetic examples from the rare or minority class, thus indirectly changing the updating weights and compensating for skewed distributions. SMOTEBoost applied to several highly and moderately \u2026",
                "A. Cllnical rationale Magnetic resonance imaging (MRI) systems measure spatial distributions of several distinct tissue-related parameters such as relaxation times and proton density. Analogous to LANDSAT imagery, MRI measurements are collections of features (that is, numerical characteristics) from spatial arrays that are aggregated into multidimen-sional data (from a single anatomical slice).\" Measured intensities at p different \u201cfrequencies\u201d at each spatial (pixel) location can be used as a basis for algorithmically formed tissue clusters in p-dimensional feature space, which is the collection of all possible aggregates of the measured features. Asp is increased using different tailored pulse sequences, higher-dimensional feature spaces may yield improved image segmentation when compared to visual interpretation or gray-scale segmentation of single",
                "\u201cRadiomics\u201d refers to the extraction and analysis of large amounts of advanced quantitative imaging features with high throughput from medical images obtained with computed tomography, positron emission tomography or magnetic resonance imaging. Importantly, these data are designed to be extracted from standard-of-care images, leading to a very large potential subject pool. Radiomics data are in a mineable form that can be used to build descriptive and predictive models relating image features to phenotypes or gene\u2013protein signatures. The core hypothesis of radiomics is that these models, which can include biological or medical data, can provide valuable diagnostic, prognostic or predictive information. The radiomics enterprise can be divided into distinct processes, each with its own challenges that need to be overcome: (a) image acquisition and reconstruction, (b) image segmentation and rendering, (c \u2026",
                "The current literature on MRI segmentation methods is reviewed. Particular emphasis is placed on the relative merits of single image versus multispectral segmentation, and supervised versus unsupervised segmentation methods. Image pre-processing and registration are discussed, as well as methods of validation. The application of MRI segmentation for tumor volume measurements during the course of therapy is presented here as an example, illustrating problems associated with inter- and intra-observer variations inherent to supervised methods.",
                "Magnetic resonance (MR) brain section images are segmented and then synthetically colored to give visual representations of the original data with three approaches: the literal and approximate fuzzy c-means unsupervised clustering algorithms, and a supervised computational neural network. Initial clinical results are presented on normal volunteers and selected patients with brain tumors surrounded by edema. Supervised and unsupervised segmentation techniques provide broadly similar results. Unsupervised fuzzy algorithms were visually observed to show better segmentation when compared with raw image data for volunteer studies. For a more complex segmentation problem with tumor\/edema or cerebrospinal fluid boundary, where the tissues have similar MR relaxation behavior, inconsistency in rating among experts was observed, with fuzz-c-means approaches being slightly preferred over feedforward \u2026",
                "A system that automatically segments and labels glioblastoma-multiforme tumors in magnetic resonance images (MRIs) of the human brain is presented. The MRIs consist of T1-weighted, proton density, and T2-weighted feature images and are processed by a system which integrates knowledge-based (KB) techniques with multispectral analysis. Initial segmentation is performed by an unsupervised clustering algorithm. The segmented image, along with cluster centers for each class are provided to a rule-based expert system which extracts the intracranial region. Multispectral histogram analysis separates suspected tumor from the rest of the intracranial region, with region analysis used in performing the final tumor labeling. This system has been trained on three volume data sets and tested on thirteen unseen volume data sets acquired from a single MRI system. The KB tumor segmentation was compared with \u2026",
                "Describes a genetically guided approach to optimizing the hard (J\/sub 1\/) and fuzzy (J\/sub m\/) c-means functionals used in cluster analysis. Our experiments show that a genetic algorithm (GA) can ameliorate the difficulty of choosing an initialization for the c-means clustering algorithms. Experiments use six data sets, including the Iris data, magnetic resonance, and color images. The genetic algorithm approach is generally able to find the lowest known J\/sub m\/ value or a J\/sub m\/ associated with a partition very similar to that associated with the lowest J\/sub m\/ value. On data sets with several local extrema, the GA approach always avoids the less desirable solutions. Degenerate partitions are always avoided by the GA approach, which provides an effective method for optimizing clustering models whose objective function can be represented in terms of cluster centers. A series random initializations of fuzzy\/hard c \u2026",
                "When clustering algorithms are applied to image segmentation, the goal is to solve a classification problem. However, these algorithms do not directly optimize classification duality. As a result, they are susceptible to two problems: 1) the criterion they optimize may not be a good estimator of \"true\" classification quality, and 2) they often admit many (suboptimal) solutions. This paper introduces an algorithm that uses cluster validity to mitigate problems 1 and 2. The validity-guided (re)clustering (VGC) algorithm uses cluster-validity information to guide a fuzzy (re)clustering process toward better solutions. It starts with a partition generated by a soft or fuzzy clustering algorithm. Then it iteratively alters the partition by applying (novel) split-and-merge operations to the clusters. Partition modifications that result in improved partition validity are retained. VGC is tested on both synthetic and real-world data. For magnetic \u2026",
                "We experimentally evaluate bagging and seven other randomization-based approaches to creating an ensemble of decision tree classifiers. Statistical tests were performed on experimental results from 57 publicly available data sets. When cross-validation comparisons were tested for statistical significance, the best method was statistically more accurate than bagging on only eight of the 57 data sets. Alternatively, examining the average ranks of the algorithms across the group of data sets, we find that boosting, random forests, and randomized trees are statistically significantly better than bagging. Because our results suggest that using an appropriate ensemble size is important, we introduce an algorithm that decides when a sufficient number of classifiers has been created for an ensemble. Our algorithm uses the out-of-bag error estimate, and is shown to result in an accurate ensemble for those methods that \u2026"
            ],
            [
                [
                    "artificial intelligence",
                    "pattern recognition",
                    "data mining",
                    "fuzzy sets"
                ]
            ],
            [
                "Lawrence Hall Lawrence Hall 413 Physics South ljhall@lbl.gov (510) 642-6536 (510) 486-4469 Lawrence Hall received his B.A. from Oxford in 1977 and his Ph.D. from Harvard in 1981. He was a Miller Fellow at Berkeley from 1981-83, and a junior faculty member at Harvard from 1983-86. He has been on the Berkeley faculty since 1986. He received Sloan and Presidential Young Investigator Awards, and is a Fellow of the American Physical Society. What are the fundamental laws of nature, and how are they determined? The standard model of particle physics, while very successful, leaves many fundamental questions unanswered. These questions frequently have to do with symmetries: could certain aspects of particles and their interactions be better understood in terms of a new symmetry? How are these new symmetries, and indeed many of the symmetries of the standard model, to be broken? The symmetry of the electroweak interaction would imply that there is no difference between the left-handed neutrino and the left-handed electron! These particles differ only because the symmetry is broken -- and yet we do not know how this symmetry breaking occurs. The difference between the masses of the electron and the muon are a sign that flavor symmetries are broken; but again, the origin is unknown. It is remarkable that we can address such problems, and in the coming decade we shall find answers to at least some of the fundamental questions of symmetry breaking. In recent years I have constructed and studied theories with enhanced spacetime, gauge and flavor symmetries: supersymmetry, and compact extra spatial dimensions with size from sub-mm to inverse TeV to inverse Planck mass. I have worked on grand unified theories in four and higher dimensions, and have considered a variety of frameworks for neutrino masses. Recently I have focused on the TeV scale, since is the range of the Large Hadron Collider. Whether the electroweak symmetry turns out to be broken by supersymmetric interactions, a new strong force, or by extra spatial dimensions, the elucidation of the TeV scale will be as exciting as any previous discoveries in particle physics. The cosmos contains both dark energy and dark matter, dominating the contents of a critical universe. These contents of the universe, as well as the asymmetric baryons and background photons, arise from the underlying theory of particle and gravitational interactions. For all its successes, the standard model of particle physics does not allow the computation of the density of any of these components of the universe. What theory will? Symmetries have carried us far, but will they take us further? Recently I have explored the extent to which the questions that face us might be answered by environmental selection from a large landscape of vacua. Current Projects What will we discover at the LHC? There are many theories but no leading candidate; my latest theory is supersymmetric with a large Higgs interaction [1]. Using the physics of extra spatial dimensions, I have constructed a supersymmetric theory with a fifth dimension at the TeV scale that is responsible for electroweak symmetry breaking [2]. This allows a prediction for the mass of the Higgs boson and for the pattern of the masses of the superpartners and KK resonances. I have introduced a new framework for grand unification, in which the unified symmetry is realized only in higher dimensions and the higher dimensional field theory allows a precise calculation of the weak mixing angle [3]. Such theories have new possibilities for understanding gauge symmetry breaking, quark and lepton masses and proton decay. Why are lepton-flavor mixing angles large while those in the quark sector are small? I am pursuing a variety of ideas to explain this quark-lepton asymmetry. Possible schemes for large mixing between the tau neutrino and the muon neutrino were explored in [4], and the possibility of a statistical understanding from the landscape in [5]. Cosmological data suggest that we live in an interesting period in the history of the universe when the matter, radiation and vacuum contributions to the energy density are broadly comparable. The occurrence of any epoch with such a \"triple coincidence\" is puzzling, and yet it appears that we happen to live during this special epoch. I have proposed why this might be so [6], and have proposed a theory that leads to such interesting times [7]. Improved naturalness with a heavy Higgs: An Alternative road to LHC physics. Riccardo Barbieri, Lawrence Hall and Vyacheslav Rychkov, Phys.Rev.D74:015007,2006. e-Print: hep-ph\/0603188. A constrained standard model from a compact extra dimension, Riccardo Barbieri, Lawrence J. Hall, and Yasunori Nomura, Phys. Rev. D63, 105007 (2001); hep-ph\/0011311. Gauge unification in higher dimensions, Lawrence Hall and Yasunori Nomura, Phys. Rev. D64, 055003 (2001); hep-ph\/0103125. Oscillations of solar and atmospheric neutrinos, R. Barbieri, L. J. Hall, D. Smith, A. Strumia, and N. Weiner, JHEP 12, 17 (1998); hep-ph\/9807235. Statistical Understanding of Quark and Lepton Masses in Gaussian Landscapes. Lawrence Hall, Michael Salem and Taizan Watari, e-Print: arXiv:0707.3446. A new perspective on cosmic coincidence problems, Nima Arkani-Hamed, Lawrence J. Hall, Christopher Kolda, and Hitoshi Murayama, PRL. 85, 4434 (2000); astro-ph\/0005111. Acceleressence: dark energy from a phase transition at the seesaw scale. Z. Chacko, Lawrence Hall and Yasunori Nomura, JCAP 0410:011,2004. e-Print: astro-ph\/0405596."
            ]
        ]
    ]
}
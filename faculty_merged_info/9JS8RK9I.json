{
    "columns":[
        "id",
        "google_scholar_id",
        "titles",
        "abstracts",
        "interests"
    ],
    "index":[
        0
    ],
    "data":[
        [
            "9JS8RK9I",
            "hiI-uAcAAAAJ",
            [
                "Learning and scientific reasoning",
                "Model analysis of fine structures of student models: An example with Newton\u2019s third law",
                "Theoretical comparisons of average normalized gain calculations"
            ],
            [
                "Data Collection and Analysis From the early 1980s, researchers and educators in psychology and cognitive science (11\u201314) have developed many quantitative instruments that assess reasoning ability. Some are included as components in standard assessments such as the Graduate Record Examination, whereas others are stand-alone tests such as Lawson\u2019s Classroom Test of Scientific Reasoning (LCTSR)(8, 9). We used the LCTSR because of its popularity among STEM educators and researchers. Common categories of reasoning ability assessments include proportional reasoning, deductive and inductive reasoning, control of variables, probability reasoning, correlation reasoning, and hypothesis evaluation, all of which are crucial skills needed for a successful career in STEM. Research-based standardized tests that assess student STEM content knowledge are also widespread. For example, in physics, education research has produced many instruments. We used the Force Concept",
                "In problem-solving situations, the contextual features of the problems affect student reasoning. Using Newton\u2019s third law as an example, we study the role of context in students\u2019 uses of alternative conceptual models. We have identified four contextual features that are frequently used by students in their reasoning. Using these results, a multiple-choice survey was developed to probe the effects of the specific contextual features on student reasoning. Measurements with this instrument show that different contextual features can affect students\u2019 conceptual learning in different ways. We compare student data from different populations and instructions and discuss the implications.",
                "Since its introduction, the normalized gain or the g-factor has been widely used in assessing students\u2019 performance in pre- and post-tests. The average g-factor can be calculated using either the average scores of the class or individual student\u2019s scores. In general, these two calculations produce different results. The nature of these two results is explored for several idealized situations. The results suggest that we may be able to utilize the difference between the two results to extract information on how the population may have changed as a result of instruction."
            ],
            [
                [
                    "Information Theory",
                    "Learning theory",
                    "Education"
                ]
            ]
        ]
    ]
}